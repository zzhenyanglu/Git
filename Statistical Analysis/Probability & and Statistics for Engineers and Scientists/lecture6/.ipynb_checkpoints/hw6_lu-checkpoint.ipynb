{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 \n",
    "\n",
    "## Zhenyang Lu\n",
    "\n",
    "\n",
    "# Question 1\n",
    "\n",
    "\n",
    "# a\n",
    "Let X be the number of plain meatballs this person has reported .\n",
    "\n",
    "Let $ \\theta $ be the number of spicy meatballs each one has.\n",
    "\n",
    "For 1 person, the process can be modeled by hypergeometric distribution whose the likeliness function of eating X plain meatballs is\n",
    "$ C_{20}^5* \\theta^{5-x} *(20-\\theta)^x $, which if taking log on it, will be $ log(C_{20}^5) + (5-x)log(\\theta)+xlog(20-\\theta)$. Then we take first derivative with respect to x, we get $x = \\frac{log(20-\\theta)}{log(\\theta)}$, which is the final answer. \n",
    "\n",
    "\n",
    "\n",
    "# b\n",
    "\n",
    "Let X_i be the number of plain meatballs each person has reported .\n",
    "\n",
    "Let $ \\theta $ be the number of spicy meatballs each one has. \n",
    "\n",
    "For each person, the process can be modeled by binomial distribution, whose p is $\\frac{(20 - \\theta)}{20}$. The likeliness function of eating X_i plain meatballs is\n",
    "$ C_{20}^{X_i} *(\\frac{ \\theta }{20})^{5-{X_i}} * (\\frac{ 20-\\theta }{20})^{X_i}$, take the derivative of the log of the PMF, we have $\\sum (5-X_i) (log \\theta - log20)+X(log(20-\\theta) - log20)$, after some algebra manipulation, we have $\\theta = 20 - 4*X $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "# a\n",
    "\n",
    "Let $\\lambda$ be the population mean of length of time until a rip happens. \n",
    "\n",
    "Let $x_i$ be each experiment's length of time until a rip happens. \n",
    "\n",
    "So lik($\\lambda$) = $\\lambda*e^{-\\lambda*x_1} * \\lambda*e^{-\\lambda*x_2} *... \\lambda*e^{-\\lambda*x_k} = \\lambda^k*e^{-\\lambda* \\sum x_i }$\n",
    "\n",
    "Taking the log of likeliness function, we have $k*log(\\lambda) - \\lambda \\sum x_i $, we have $\\lambda = \\frac{1}{ \\overline X} $ \n",
    "\n",
    "\n",
    "# b\n",
    "\n",
    "Let $\\lambda$ be the population mean of length of time until a rip happens. \n",
    "\n",
    "Let $x_i$ be each experiment's length of time until a rip happens, which follows uniform distribution $[0,\\theta]$\n",
    "\n",
    "$like(\\theta) = \\frac{1}{\\theta^n} $, according to example 6.3-6 on textbook P241, the MLE of the upper bound in a uniform distributed sample is the largest value in a sample. In this problem, the largest value of the experiments depends on if all boxes have been ripped or not. If there is, then the MLE of $\\theta = x_n$, if not $\\theta = H$. \n",
    "\n",
    "Since mean of uniform distribution is $\\frac{\\theta}{2}$, the MLE for mean can be summarized as: \n",
    "\n",
    "$ \\frac{x_n}{2}$ if $ x_n \\leq H $ \n",
    "\n",
    "$ \\frac{H}{2}$ if $ x_n \\geq H $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "\n",
    "let $\\sigma^2 $ denotes the variance.\n",
    "\n",
    "normal distribution pdf log likeliness function = $ -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(x_i - \\mu)^2}{2 \\sigma^2} $\n",
    "\n",
    "\n",
    "\n",
    "So\n",
    "\n",
    "$ log(lik(\\mu_1*\\mu_2*\\mu)) =  -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(x_1 - \\mu_1)^2}{2 \\sigma^2} +  -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(x_2- \\mu_1)^2}{2 \\sigma^2} + ... + -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(x_n - \\mu_1)^2}{2 \\sigma^2}  +  \n",
    "-\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(y_1 - \\mu_1)^2}{2 \\sigma^2} +  -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(y_2- \\mu_1)^2}{2 \\sigma^2} + ... + -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(y_n - \\mu_1)^2}{2 \\sigma^2} +\n",
    "-\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(g_1 - \\mu_1)^2}{2 \\sigma^2} +  -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(g_2- \\mu_1)^2}{2 \\sigma^2} + ... + -\\frac{1}{2}\\log(2\\pi)-\\log (\\sigma) - \\frac{(g_n - \\mu_1)^2}{2 \\sigma^2}$\n",
    "\n",
    "\n",
    "# a\n",
    "\n",
    "Taking the derivative with respect to $\\mu_1$, we have $\\frac{\\overline X - \\mu_1 + \\overline G - \\mu_1 + \\mu_2 }{\\sigma^2} = 0 $. Ignoring the details of manipulation, we have $\\mu_1 = \\frac{\\overline G  + \\overline X + \\mu_2}{2} $\n",
    "\n",
    "\n",
    "# b\n",
    "\n",
    "Applying the same method and taking derivative with respect to $\\mu_1$, we have  $\\frac{\\overline Y + \\mu_1 - \\overline G - \\mu_2 - \\mu_2 }{\\sigma^2} = 0 $, $\\mu_2 = \\frac{\\overline Y  - \\overline G + \\mu_1}{2} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qestion 4\n",
    "\n",
    "# a\n",
    "\n",
    "Geometric(p)'s mean equals $\\frac{1}{p}$. Assuming sample mean is $\\overline X$, then $\\overline X = \\frac{1}{p}$, which means $ \\hat p = \\frac{1}{\\overline X }$  Where $\\bar{X} = \\frac{\\sum_{i=1}^{n}X_i}{n}$\n",
    "\n",
    "# b\n",
    "\n",
    "$log(P(X=k)) = log(p) + (k-1)log(1-p)$\n",
    "\n",
    "$log-lik(p) = nlog(p) + (\\sum(x_i-1))*log(1-p) $ whose derivative  is $\\frac{n}{p} - \\frac{\\sum(x_i-1)}{1-p} $, when this equation equals to 0, we have $\\frac{1}{p} = \\frac{\\overline X-1}{1-p}$, which means $\\hat p = \\frac{1}{\\overline X }$, Where $\\bar{X} = \\frac{\\sum_{i=1}^{n}X_i}{n}$\n",
    "\n",
    "\n",
    "# c \n",
    "\n",
    "According to Part a and b, and geometric distribution's mean we have $bias(\\hat p) = E(\\hat p) - p = 0$. So we have $MSE(\\hat p) = \\sigma _{\\hat p}^2 $\n",
    "\n",
    "this specific question needs some knowledge that we don't cover on this course, which is Fisher Information. Otherwise, we could use empirical way to solving it by do experiments. Just joggin down the choices to solve it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 5\n",
    "\n",
    "\n",
    "\n",
    "# a.\n",
    "\n",
    "$ E(x) = \\int_{+\\infty}^{-\\infty} x*f(x) dx = \\int \\frac{x}{2*\\sigma}*e^{-\\frac{|x|}{\\sigma} } dx= -\\frac{e^{-\\frac{|x|}{\\sigma}}}{2} |_{+\\infty}^{-\\infty} = 0 $\n",
    "\n",
    "$ E(x^2) = \\int_{+\\infty}^{-\\infty} x^2*f(x) dx = \\int x^2 \\frac{1}{\\sigma}*e^{(-x/\\sigma)} $. To get rid of the |x|, we need to integrate it by two part, first from negative infinity to 0, second from 0 to positive infinity. \n",
    " \n",
    "$ E(x^2) = \\int_{0}^{-\\infty} x^2*f(x) dx + \\int_{+\\infty}^{0} x^2*f(x) dx$. \n",
    "\n",
    "Pardon me for ignoring 14 intermediate steps. This integral part can be done by integrating by parts twice, and finally we get $ E(x^2) = 2*(-\\frac{x}{2e^{|x|/\\sigma}}-\\frac{x\\sigma}{e{|x|/\\sigma}} -\\frac{\\sigma^2}{e^{|x|/\\sigma}})|^{+\\infty}_0 = 2\\sigma^2$\n",
    "\n",
    "So to continue, $E(X_i^2) = 2\\sigma^2$, which means $\\sigma = \\sqrt{E(X_i^2)/2}$.\n",
    "\n",
    "\n",
    "# b.\n",
    "\n",
    "$log(f(x|\\sigma)) = log(\\frac{1}{2\\sigma}) - \\frac{|x|}{\\sigma}$\n",
    "\n",
    "$log-lik(\\sigma) = n*log(\\frac{1}{2\\sigma}) -n*\\frac{\\sum |x|}{\\sigma} $. \n",
    "\n",
    "Taking the first derivative od the log-lik function we have $-\\frac{n}{\\sigma} + \\frac{n|\\overline X|}{\\sigma^2} = 0$, therefore, $\\sigma = \\overline X$, which is the sample mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# question 6. \n",
    "\n",
    "# a. \n",
    "\n",
    "Pardon me for ignoring details of the steps of integral. just so tired of doing so. \n",
    "\n",
    "$ E(X) = \\int x*e^{\\theta - x} dx$, this can be solved by integral-by-parts method. and finally we have $ E(X) = \\int x*e^{\\theta - x} dx = -e^{(\\theta - x)}*(x+1) |^{+\\infty}_0 =e^\\theta $, so we have $E(X) = \\overline X = e^\\theta$, $\\theta = log(\\overline X)$\n",
    "\n",
    "\n",
    "# b.\n",
    "\n",
    "$ log(f(x|\\theta) = \\theta - X_i$, i = 0,1..,n\n",
    "\n",
    "So log-lik($\\theta$) = $n*\\theta - n*\\overline X$, we can't get max value by taking first derivative because doing so will leave us only a n, namely $\\frac{d(log-lik(\\theta))}{d\\theta} = n $. But the log-like function can be maximized by getting $\\theta$ as large as possible. So we could have $\\theta = max(x_i)$ , which is the largest possible $x_i$, because $x_i \\geq \\theta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7.\n",
    "\n",
    "# a\n",
    "\n",
    "According to uniform distribution's property. we have $$\\hat \\alpha = \\overline X - \\sqrt{3S^2} = 28.4738$$\n",
    "\n",
    "$$\\hat \\beta = \\overline X + \\sqrt{3S^2} = 32.57074$$\n",
    "\n",
    "Notice: I hard copied t1 into this notebook, so t1 can be used directly by calling t1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "28.4738033905785"
      ],
      "text/latex": [
       "28.4738033905785"
      ],
      "text/markdown": [
       "28.4738033905785"
      ],
      "text/plain": [
       "[1] 28.4738"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "32.570742063967"
      ],
      "text/latex": [
       "32.570742063967"
      ],
      "text/markdown": [
       "32.570742063967"
      ],
      "text/plain": [
       "[1] 32.57074"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code used to get sample mean and variance \n",
    "t1=c(29.59 ,29.25 ,31.41, 29.84 ,30.76 ,31.19 ,31.60 ,31.90, 30.28,29.06,29.76, 31.55, 32.42, 31.01 ,30.34 ,30.84 ,32.74 ,30.03 ,28.35 ,28.98,31.27,29.32)\n",
    "\n",
    "# alpha\n",
    "mean(t1) - sqrt(3*var(t1))\n",
    "\n",
    "# beta\n",
    "mean(t1) + sqrt(3*var(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b\n",
    "\n",
    "ignoring the intermediate steps, MLE $\\hat \\lambda = \\frac{1}{\\overline X} = 0.03276296$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0327629599845121"
      ],
      "text/latex": [
       "0.0327629599845121"
      ],
      "text/markdown": [
       "0.0327629599845121"
      ],
      "text/plain": [
       "[1] 0.03276296"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code to get part b \n",
    "\n",
    "1/mean(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C\n",
    "\n",
    "using uniform(28.473803,32.570742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "31.751352"
      ],
      "text/latex": [
       "31.751352"
      ],
      "text/markdown": [
       "31.751352"
      ],
      "text/plain": [
       "[1] 31.75135"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.311500778629904"
      ],
      "text/latex": [
       "0.311500778629904"
      ],
      "text/markdown": [
       "0.311500778629904"
      ],
      "text/plain": [
       "[1] 0.3115008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# modeling with uniform(28.473803,32.570742)\n",
    "# 80% percentile \n",
    "qunif(0.8,28.4738,32.57074)\n",
    "punif(29.75,28.4738,32.57074)-punif(28.15,28.4738,32.57074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "49.123702900926"
      ],
      "text/latex": [
       "49.123702900926"
      ],
      "text/markdown": [
       "49.123702900926"
      ],
      "text/plain": [
       "[1] 49.1237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0203062565059107"
      ],
      "text/latex": [
       "0.0203062565059107"
      ],
      "text/markdown": [
       "0.0203062565059107"
      ],
      "text/plain": [
       "[1] 0.02030626"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# modeling with exponential(0.0327629599845121)\n",
    "# 80% percentile \n",
    "qexp(.8,0.0327629599845121)\n",
    "pexp(29.75,rate=0.0327629599845121)-pexp(28.15,rate=0.0327629599845121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d\n",
    "\n",
    "80% is the 17.6th item in t1, so I'm extrapolating linearly to get it, that is t1[17] + 0.6*(t1[18]-t1[17]). In t1's case, it is 31.494. \n",
    "\n",
    "distribution function counts how many items in t1 are less than or equal to a given value x, and then divides the count by length(t1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>80%:</strong> 31.522"
      ],
      "text/latex": [
       "\\textbf{80\\textbackslash{}\\%:} 31.522"
      ],
      "text/markdown": [
       "**80%:** 31.522"
      ],
      "text/plain": [
       "   80% \n",
       "31.522 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "31.494"
      ],
      "text/latex": [
       "31.494"
      ],
      "text/markdown": [
       "31.494"
      ],
      "text/plain": [
       "[1] 31.494"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.272727272727273"
      ],
      "text/latex": [
       "0.272727272727273"
      ],
      "text/markdown": [
       "0.272727272727273"
      ],
      "text/plain": [
       "[1] 0.2727273"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# two versions of quantile function: \n",
    "\n",
    "q=function(q,v,extrapolate=TRUE)\n",
    "{\n",
    "     if (extrapolate==FALSE){return(quantile(v,0.8))}\n",
    "     t=sort(v);\n",
    "     t[floor(q*length(t))]+ +\n",
    "     (length(t)*q-floor(q*length(t)))* +\n",
    "     (t[ceiling(q*length(t))]-t[floor(q*length(t))]);\n",
    "}\n",
    "\n",
    "q(q=0.8,v=t1,extrapolate=FALSE)\n",
    "q(q=0.8,v=t1,extrapolate=TRUE)\n",
    "\n",
    "# distribution function: \n",
    "p=function(v,x)\n",
    "{\n",
    "  t=sort(v);\n",
    "  count=0;\n",
    "\n",
    "  for (i in t){if(i<=x){count=count+1;}}\n",
    "  \n",
    "  count/length(t);\n",
    "}\n",
    "\n",
    "p(t1,29.75) - p(t1,28.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
