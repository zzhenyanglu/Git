{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9 Songhao Jiang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "\n",
    "$E[X_1X_2] = Cov(X_1, X_2) + E[X_1]E[X_2] = 80 + 10\\cdot18 = 260$\n",
    "\n",
    "$\\Rightarrow E[Y] = 3.6 + 2.7 \\cdot E[X_1] + 0.9 \\cdot E[X_2] + 1.5 \\cdot E[X_1X_2] = 436.8$\n",
    "\n",
    "### 2)\n",
    "\n",
    "Here $\\beta_3$ is the coeeficient of the term $X_1X_2$. Compare the centered variable terms equation to the original equation wii give $\\beta_3 = 1.5$\n",
    "\n",
    "Take expectation on both sides $\\Rightarrow E[Y] = \\beta_0 + \\beta_1 \\cdot E[X_1-\\mu_{X_1}] + \\beta_2 \\cdot E[X_2-\\mu_{X_2}] + \\beta_3 \\cdot E[(X_1-\\mu_{X_1})(X_2-\\mu_{X_2})]$\n",
    "\n",
    "$= \\beta_0 + \\beta_3 Cov(X_1, X_2) = 316.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "436.8"
      ],
      "text/latex": [
       "436.8"
      ],
      "text/markdown": [
       "436.8"
      ],
      "text/plain": [
       "[1] 436.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "316.8"
      ],
      "text/latex": [
       "316.8"
      ],
      "text/markdown": [
       "316.8"
      ],
      "text/plain": [
       "[1] 316.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "3.6+2.7*10+0.9*18+1.5*260\n",
    "436.8 - 1.5*80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=stackloss$stack.loss\n",
    "x1=stackloss$Air.Flow\n",
    "x2=stackloss$Water.Temp\n",
    "x3=stackloss$Acid.Conc.\n",
    "x1=x1-mean(x1)\n",
    "x2=x2-mean(x2)\n",
    "x3=x3-mean(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ x1 + x2 + x3)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-7.2377 -1.7117 -0.4551  2.3614  5.6978 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  17.5238     0.7078  24.760 8.91e-15 ***\n",
       "x1            0.7156     0.1349   5.307 5.80e-05 ***\n",
       "x2            1.2953     0.3680   3.520  0.00263 ** \n",
       "x3           -0.1521     0.1563  -0.973  0.34405    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 3.243 on 17 degrees of freedom\n",
       "Multiple R-squared:  0.9136,\tAdjusted R-squared:  0.8983 \n",
       "F-statistic:  59.9 on 3 and 17 DF,  p-value: 3.016e-09\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit1 = lm(y~x1+x2+x3)\n",
    "summary(fit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = 17.5238 + 0.7156X_1 + 1.2953X_2 - 0.1521X_3$\n",
    "\n",
    "adjusted $R^2 = 0.8983$\n",
    "\n",
    "p-value $= 3.016\\cdot 10^{-9}$, which meas that the model is useful for predicting stackloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ x1 + x2)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-7.5290 -1.7505  0.1894  2.1156  5.6588 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  17.5238     0.7067  24.796 2.29e-15 ***\n",
       "x1            0.6712     0.1267   5.298 4.90e-05 ***\n",
       "x2            1.2954     0.3675   3.525  0.00242 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 3.239 on 18 degrees of freedom\n",
       "Multiple R-squared:  0.9088,\tAdjusted R-squared:  0.8986 \n",
       "F-statistic: 89.64 on 2 and 18 DF,  p-value: 4.382e-10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit2 = lm(y~x1+x2)\n",
    "summary(fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value for $x_3$ is 0.34405, thus it is not a useful predictor in the model\n",
    "\n",
    "new adjusted $R^2 = 0.8986$ which is very close to the value in part a. And therefore it is consistent with the conclusion that $x_3$ is not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>32.59631</td><td>25.22471</td><td>39.96792</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & fit & lwr & upr\\\\\n",
       "\\hline\n",
       "\t1 & 32.59631 & 25.22471 & 39.96792\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | fit | lwr | upr | \n",
       "|---|\n",
       "| 1 | 32.59631 | 25.22471 | 39.96792 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  fit      lwr      upr     \n",
       "1 32.59631 25.22471 39.96792"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(fit2, data.frame(x1=85-mean(stackloss$Air.Flow), x2=20-mean(stackloss$Water.Temp)), interval = \"confidence\", level = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with only $x_1$ and $x_2$ has nothing to do with acid concentration, and it only have water temprature as its predictor. And therefore we will get a list of predictions for stackloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ poly(x1, 2, raw = T) + poly(x2, 2, raw = T) + \n",
       "    x1:x2)\n",
       "\n",
       "Residuals:\n",
       "   Min     1Q Median     3Q    Max \n",
       "-3.710 -1.645 -0.395  1.290  6.355 \n",
       "\n",
       "Coefficients:\n",
       "                      Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)           17.19405    1.57043  10.949  1.5e-08 ***\n",
       "poly(x1, 2, raw = T)1  0.62793    0.15478   4.057  0.00103 ** \n",
       "poly(x1, 2, raw = T)2 -0.03857    0.02370  -1.628  0.12444    \n",
       "poly(x2, 2, raw = T)1  1.12312    0.36044   3.116  0.00708 ** \n",
       "poly(x2, 2, raw = T)2 -0.06624    0.23901  -0.277  0.78544    \n",
       "x1:x2                  0.18757    0.11824   1.586  0.13350    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.831 on 15 degrees of freedom\n",
       "Multiple R-squared:  0.9419,\tAdjusted R-squared:  0.9226 \n",
       "F-statistic: 48.65 on 5 and 15 DF,  p-value: 9.663e-09\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit3 = lm(y~poly(x1,2,raw=T)+poly(x2,2,raw=T)+x1:x2)\n",
    "summary(fit3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>18        </td><td>188.7953  </td><td>NA        </td><td>      NA  </td><td>      NA  </td><td>        NA</td></tr>\n",
       "\t<tr><td>15        </td><td>120.1911  </td><td> 3        </td><td>68.60424  </td><td>2.853965  </td><td>0.07237802</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n",
       "\\hline\n",
       "\t 18         & 188.7953   & NA         &       NA   &       NA   &         NA\\\\\n",
       "\t 15         & 120.1911   &  3         & 68.60424   & 2.853965   & 0.07237802\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) | \n",
       "|---|---|\n",
       "| 18         | 188.7953   | NA         |       NA   |       NA   |         NA | \n",
       "| 15         | 120.1911   |  3         | 68.60424   | 2.853965   | 0.07237802 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Res.Df RSS      Df Sum of Sq F        Pr(>F)    \n",
       "1 18     188.7953 NA       NA        NA         NA\n",
       "2 15     120.1911  3 68.60424  2.853965 0.07237802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anova(fit2, fit3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = 17.19405 + 0.62793x_1 - 0.03857x_1^2+1.12312x_2-0.06624x_2^2+0.18757x_1x_2$\n",
    "\n",
    "The p-value $=0.07238 > 0.05$, and therefore we cannot reject the null hypothesis, which implies that the two quadratic terms and the interaction are not significant at level $0.05$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nrkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs(stackloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph Acid.Conc appear correlated with stackloss.\n",
    "\n",
    "The high value maybe becuase that the Acid.Conc is explained by Air.Flow and Water.Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true model is the following polynomial of degree 4.\n",
    "\n",
    "We will build samples for polynomial regressions by adding random noise to the base polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base.poly <- function(x) 0.005*x^4 - 0.28*x^3 + 2.36*x^2 - 5.6*x + 2000\n",
    "x <- 1:50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Construct a vector \"y\" such that, for 1 <= i <= 50, y[i] is equal to base.poly(i) plus a random error term taken from a normal distribution with mean=0 and sd=400.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1996.485</li>\n",
       "\t<li>1996.08</li>\n",
       "\t<li>1997.285</li>\n",
       "\t<li>1998.72</li>\n",
       "\t<li>1999.125</li>\n",
       "\t<li>1997.36</li>\n",
       "\t<li>1992.405</li>\n",
       "\t<li>1983.36</li>\n",
       "\t<li>1969.445</li>\n",
       "\t<li>1950</li>\n",
       "\t<li>1924.485</li>\n",
       "\t<li>1892.48</li>\n",
       "\t<li>1853.685</li>\n",
       "\t<li>1807.92</li>\n",
       "\t<li>1755.125</li>\n",
       "\t<li>1695.36</li>\n",
       "\t<li>1628.805</li>\n",
       "\t<li>1555.76</li>\n",
       "\t<li>1476.645</li>\n",
       "\t<li>1392</li>\n",
       "\t<li>1302.485</li>\n",
       "\t<li>1208.88</li>\n",
       "\t<li>1112.085</li>\n",
       "\t<li>1013.12</li>\n",
       "\t<li>913.125</li>\n",
       "\t<li>813.359999999999</li>\n",
       "\t<li>715.204999999999</li>\n",
       "\t<li>620.16</li>\n",
       "\t<li>529.844999999999</li>\n",
       "\t<li>445.999999999999</li>\n",
       "\t<li>370.484999999999</li>\n",
       "\t<li>305.279999999999</li>\n",
       "\t<li>252.485</li>\n",
       "\t<li>214.319999999999</li>\n",
       "\t<li>193.124999999998</li>\n",
       "\t<li>191.359999999998</li>\n",
       "\t<li>211.604999999998</li>\n",
       "\t<li>256.559999999998</li>\n",
       "\t<li>329.044999999996</li>\n",
       "\t<li>432</li>\n",
       "\t<li>568.484999999999</li>\n",
       "\t<li>741.679999999996</li>\n",
       "\t<li>954.884999999998</li>\n",
       "\t<li>1211.52</li>\n",
       "\t<li>1515.125</li>\n",
       "\t<li>1869.36</li>\n",
       "\t<li>2278.005</li>\n",
       "\t<li>2744.96</li>\n",
       "\t<li>3274.245</li>\n",
       "\t<li>3870</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1996.485\n",
       "\\item 1996.08\n",
       "\\item 1997.285\n",
       "\\item 1998.72\n",
       "\\item 1999.125\n",
       "\\item 1997.36\n",
       "\\item 1992.405\n",
       "\\item 1983.36\n",
       "\\item 1969.445\n",
       "\\item 1950\n",
       "\\item 1924.485\n",
       "\\item 1892.48\n",
       "\\item 1853.685\n",
       "\\item 1807.92\n",
       "\\item 1755.125\n",
       "\\item 1695.36\n",
       "\\item 1628.805\n",
       "\\item 1555.76\n",
       "\\item 1476.645\n",
       "\\item 1392\n",
       "\\item 1302.485\n",
       "\\item 1208.88\n",
       "\\item 1112.085\n",
       "\\item 1013.12\n",
       "\\item 913.125\n",
       "\\item 813.359999999999\n",
       "\\item 715.204999999999\n",
       "\\item 620.16\n",
       "\\item 529.844999999999\n",
       "\\item 445.999999999999\n",
       "\\item 370.484999999999\n",
       "\\item 305.279999999999\n",
       "\\item 252.485\n",
       "\\item 214.319999999999\n",
       "\\item 193.124999999998\n",
       "\\item 191.359999999998\n",
       "\\item 211.604999999998\n",
       "\\item 256.559999999998\n",
       "\\item 329.044999999996\n",
       "\\item 432\n",
       "\\item 568.484999999999\n",
       "\\item 741.679999999996\n",
       "\\item 954.884999999998\n",
       "\\item 1211.52\n",
       "\\item 1515.125\n",
       "\\item 1869.36\n",
       "\\item 2278.005\n",
       "\\item 2744.96\n",
       "\\item 3274.245\n",
       "\\item 3870\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1996.485\n",
       "2. 1996.08\n",
       "3. 1997.285\n",
       "4. 1998.72\n",
       "5. 1999.125\n",
       "6. 1997.36\n",
       "7. 1992.405\n",
       "8. 1983.36\n",
       "9. 1969.445\n",
       "10. 1950\n",
       "11. 1924.485\n",
       "12. 1892.48\n",
       "13. 1853.685\n",
       "14. 1807.92\n",
       "15. 1755.125\n",
       "16. 1695.36\n",
       "17. 1628.805\n",
       "18. 1555.76\n",
       "19. 1476.645\n",
       "20. 1392\n",
       "21. 1302.485\n",
       "22. 1208.88\n",
       "23. 1112.085\n",
       "24. 1013.12\n",
       "25. 913.125\n",
       "26. 813.359999999999\n",
       "27. 715.204999999999\n",
       "28. 620.16\n",
       "29. 529.844999999999\n",
       "30. 445.999999999999\n",
       "31. 370.484999999999\n",
       "32. 305.279999999999\n",
       "33. 252.485\n",
       "34. 214.319999999999\n",
       "35. 193.124999999998\n",
       "36. 191.359999999998\n",
       "37. 211.604999999998\n",
       "38. 256.559999999998\n",
       "39. 329.044999999996\n",
       "40. 432\n",
       "41. 568.484999999999\n",
       "42. 741.679999999996\n",
       "43. 954.884999999998\n",
       "44. 1211.52\n",
       "45. 1515.125\n",
       "46. 1869.36\n",
       "47. 2278.005\n",
       "48. 2744.96\n",
       "49. 3274.245\n",
       "50. 3870\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 1996.485 1996.080 1997.285 1998.720 1999.125 1997.360 1992.405 1983.360\n",
       " [9] 1969.445 1950.000 1924.485 1892.480 1853.685 1807.920 1755.125 1695.360\n",
       "[17] 1628.805 1555.760 1476.645 1392.000 1302.485 1208.880 1112.085 1013.120\n",
       "[25]  913.125  813.360  715.205  620.160  529.845  446.000  370.485  305.280\n",
       "[33]  252.485  214.320  193.125  191.360  211.605  256.560  329.045  432.000\n",
       "[41]  568.485  741.680  954.885 1211.520 1515.125 1869.360 2278.005 2744.960\n",
       "[49] 3274.245 3870.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base.poly(1:50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y <- base.poly(1:50) + rnorm(50, mean=0, sd = 400)\n",
    "samples <- data.frame(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take 10 evenly spaced samples to train our polynomial model, and leave the remaining 40 to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.rows <- seq(1,50,5)\n",
    "training <- samples[training.rows,]\n",
    "test <- samples[-training.rows,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>x</th><th scope=col>y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> 1        </td><td>1740.73051</td></tr>\n",
       "\t<tr><th scope=row>6</th><td> 6        </td><td>1991.30252</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>11        </td><td>1825.14528</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>16        </td><td>1456.05615</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>21        </td><td>1837.95995</td></tr>\n",
       "\t<tr><th scope=row>26</th><td>26        </td><td> -68.05398</td></tr>\n",
       "\t<tr><th scope=row>31</th><td>31        </td><td> 601.62164</td></tr>\n",
       "\t<tr><th scope=row>36</th><td>36        </td><td> 179.72173</td></tr>\n",
       "\t<tr><th scope=row>41</th><td>41        </td><td> 682.44076</td></tr>\n",
       "\t<tr><th scope=row>46</th><td>46        </td><td>1383.30062</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & x & y\\\\\n",
       "\\hline\n",
       "\t1 &  1         & 1740.73051\\\\\n",
       "\t6 &  6         & 1991.30252\\\\\n",
       "\t11 & 11         & 1825.14528\\\\\n",
       "\t16 & 16         & 1456.05615\\\\\n",
       "\t21 & 21         & 1837.95995\\\\\n",
       "\t26 & 26         &  -68.05398\\\\\n",
       "\t31 & 31         &  601.62164\\\\\n",
       "\t36 & 36         &  179.72173\\\\\n",
       "\t41 & 41         &  682.44076\\\\\n",
       "\t46 & 46         & 1383.30062\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | x | y | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 |  1         | 1740.73051 | \n",
       "| 6 |  6         | 1991.30252 | \n",
       "| 11 | 11         | 1825.14528 | \n",
       "| 16 | 16         | 1456.05615 | \n",
       "| 21 | 21         | 1837.95995 | \n",
       "| 26 | 26         |  -68.05398 | \n",
       "| 31 | 31         |  601.62164 | \n",
       "| 36 | 36         |  179.72173 | \n",
       "| 41 | 41         |  682.44076 | \n",
       "| 46 | 46         | 1383.30062 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   x  y         \n",
       "1   1 1740.73051\n",
       "6   6 1991.30252\n",
       "11 11 1825.14528\n",
       "16 16 1456.05615\n",
       "21 21 1837.95995\n",
       "26 26  -68.05398\n",
       "31 31  601.62164\n",
       "36 36  179.72173\n",
       "41 41  682.44076\n",
       "46 46 1383.30062"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "\n",
    "Construct a list (not a vector) \"models\" such that, for 1 <= n <= 9, models[[n]] is the linear regression model given independent variables x, x^2, x^3, ..., x^n, and dependent variable y. Use only the data found in the training set (but don't use the \"subset\" input variable, as it doesn't seem to behave properly with this problem). \n",
    "\n",
    "Hint: There are two main strategies to do this. You can construct the formulas by hand, as in the following: lm(y ~ x + I(x^2) + I(x^3), training) \n",
    "\n",
    "The I()'s are necessary.\n",
    "\n",
    "Or, you can use the poly() function in the formula, which will allow you to use a for-loop or lapply() to construct the formulas automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models=list() \n",
    "models[[1]] = lm(y~x, training)\n",
    "models[[2]] = lm(y~x+I(x^2), training)\n",
    "models[[3]] = lm(y~x+I(x^2)+I(x^3), training)\n",
    "models[[4]] = lm(y~x+I(x^2)+I(x^3)+I(x^4), training)\n",
    "models[[5]] = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5), training)\n",
    "models[[6]] = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6), training)\n",
    "models[[7]] = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7), training)\n",
    "models[[8]] = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8), training)\n",
    "models[[9]] = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9), training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Using \"models\" and the predict() function, construct the vectors training.error and test.error such that, for 1 <= n <= 9, training.error[n] and test.error[n] are the mean of the squared residuals of the training set and the mean of the squared residuals of the test set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.error <- rep(0,9)\n",
    "test.error <- rep(0,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = training$x\n",
    "test_x = test$x\n",
    "train_y = training$y\n",
    "test_y = test$y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (i in 1:9) {\n",
    "    error = 0\n",
    "    predict = models[[i]]$coef[1]\n",
    "    for (j in 1:i){\n",
    "        predict = predict + models[[i]]$coef[j+1] * I(train_x^j)\n",
    "    }\n",
    "    training.error[i] = sum((train_y - predict)^2)/10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (i in 1:9) {\n",
    "    error = 0\n",
    "    predict = models[[i]]$coef[1]\n",
    "    for (j in 1:i){\n",
    "        predict = predict + models[[i]]$coef[j+1] * I(test_x^j)\n",
    "    }\n",
    "    test.error[i] = sum((test_y - predict)^2)/40\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Look at the tables of training error and test error, and the graphs of the polynomial models compared to the training set. The training set is colored red, and the test set is colored black. How does the training error behave? How does the test error behave? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in cbind(training.error, test.error): 找不到对象'training.error'\n",
     "output_type": "error",
     "traceback": [
      "Error in cbind(training.error, test.error): 找不到对象'training.error'\nTraceback:\n",
      "1. print(cbind(training.error, test.error))",
      "2. cbind(training.error, test.error)"
     ]
    }
   ],
   "source": [
    "print(cbind(training.error,test.error))\n",
    "par(ask=T)\n",
    "xs <- seq(0,50,0.1)\n",
    "for(i in 1:9) {\n",
    "  plot(training,main=paste(\"Degree\",i,\"polynomial model\"),col=\"red\")\n",
    "  points(test)\n",
    "  lines(xs,predict(models[[i]],data.frame(x=xs)),col=\"red\")\n",
    "}\n",
    "par(ask=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>797191.087994731</li>\n",
       "\t<li>525053.053924605</li>\n",
       "\t<li>153714.743001023</li>\n",
       "\t<li>145970.826436836</li>\n",
       "\t<li>299313.376017541</li>\n",
       "\t<li>403496.933229074</li>\n",
       "\t<li>439309.210981583</li>\n",
       "\t<li>1008782.62379006</li>\n",
       "\t<li>1831900470.16633</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 797191.087994731\n",
       "\\item 525053.053924605\n",
       "\\item 153714.743001023\n",
       "\\item 145970.826436836\n",
       "\\item 299313.376017541\n",
       "\\item 403496.933229074\n",
       "\\item 439309.210981583\n",
       "\\item 1008782.62379006\n",
       "\\item 1831900470.16633\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 797191.087994731\n",
       "2. 525053.053924605\n",
       "3. 153714.743001023\n",
       "4. 145970.826436836\n",
       "5. 299313.376017541\n",
       "6. 403496.933229074\n",
       "7. 439309.210981583\n",
       "8. 1008782.62379006\n",
       "9. 1831900470.16633\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]     797191.1     525053.1     153714.7     145970.8     299313.4\n",
       "[6]     403496.9     439309.2    1008782.6 1831900470.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abs(training.error-test.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as degree increases, the training error decrease continuously. The testing error first decreases and then increases. \n",
    "\n",
    "For the training error, as the degree increases, the model become more complicated and can capture more noises in the training set. And therefore the training error continuously decrease.\n",
    "\n",
    "However, for testing error, when degree < 4, the model is underfit, and as the degree increasing, the model will generalize the data better and therefore the testing error will decrease. \n",
    "\n",
    "However, when degree comes > 4, the model will become overfit and fail to generalize the data. And therefore the testing error increase continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XC 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "set $h_{\\theta}(x^{(i)}) = \\theta_0x_0^{(i)} + \\theta_1x_1^{(i)} + \\dots + \\theta_dx_d^{(i)}$\n",
    "\n",
    "In this way $J(\\theta) = \\frac{1}{2} \\sum_{i=0}^{n}(h_{\\theta}(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "$\\frac{\\partial J(\\theta_0, \\dots \\theta_d)}{\\partial \\theta_j}$\n",
    "\n",
    "$=\\frac{1}{2} \\cdot 2 \\cdot \\sum_{i=0}^{n}(h_{\\theta}(x^{(i)})-y^{(i)}) \\cdot \\frac{\\partial}{\\partial \\theta_j}h_{\\theta}(x^{(i)})$ \n",
    "\n",
    "by definition of $h_{\\theta}(x^{(i)})$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j}h_{\\theta}(x^{(i)}) = x_j^{(i)}$\n",
    "\n",
    "And therefore $\\frac{\\partial J(\\theta_0, \\dots \\theta_d)}{\\partial \\theta_j} = \\sum_{i=0}^{n}(h_{\\theta}(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "\n",
    "\n",
    "$J(\\theta_0 \\dots \\theta_d) = \\frac{1}{2}\\sum_{i=1}^{n}(\\theta_0x_0^{(i)} + \\theta_1x_1^{(i)} + \\dots + \\theta_dx_d^{(i)} - y^{(i)})^2$\n",
    "\n",
    "$= J(\\theta_0 \\dots \\theta_d) = \\frac{1}{2}\\sum_{i=1}^{n}( \\begin{bmatrix}\n",
    "    x_0^{(i)} & x_1^{(i)} & x_2^{(i)} & \\dots & x_d^{(i)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    \\theta_0 \\\\\n",
    "    \\theta_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_d\n",
    "\\end{bmatrix} - y^{(i)})^2 $\n",
    "\n",
    "$= J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n}((x^{(i)})^T\\theta - y^{(i)})^2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XC 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mail <- read.csv(\"mail.csv\") \n",
    "X <- as.matrix(mail[,c(\"pass_vs_block\", \"recipients\", \"sanitation_checks\", \"spamscore\")])\n",
    "y <- mail$percent_spam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J = function(X, y,theta) {\n",
    "    return  (sum((X %*% theta - y)^2)/2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad.desc <- function(x,y, alpha, epsilon){\n",
    "    theta = rep(0, 5)\n",
    "    x = cbind(rep(1, length(y)), x)\n",
    "    cost = J(x,y,theta)\n",
    "    repeat {\n",
    "        theta <- theta + alpha * (t(x) %*% (y-(x %*% theta)))\n",
    "        new_cost = J(x, y, theta)\n",
    "        if(abs(new_cost - cost) < epsilon) {\n",
    "            break\n",
    "        }\n",
    "        cost = new_cost\n",
    "    }\n",
    "    \n",
    "    return(theta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td>2.419803e-04</td></tr>\n",
       "\t<tr><th scope=row>pass_vs_block</th><td>1.534096e-05</td></tr>\n",
       "\t<tr><th scope=row>recipients</th><td>2.413437e-04</td></tr>\n",
       "\t<tr><th scope=row>sanitation_checks</th><td>3.180167e-03</td></tr>\n",
       "\t<tr><th scope=row>spamscore</th><td>9.798224e-04</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t & 2.419803e-04\\\\\n",
       "\tpass\\_vs\\_block & 1.534096e-05\\\\\n",
       "\trecipients & 2.413437e-04\\\\\n",
       "\tsanitation\\_checks & 3.180167e-03\\\\\n",
       "\tspamscore & 9.798224e-04\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  | 2.419803e-04 | \n",
       "| pass_vs_block | 1.534096e-05 | \n",
       "| recipients | 2.413437e-04 | \n",
       "| sanitation_checks | 3.180167e-03 | \n",
       "| spamscore | 9.798224e-04 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                  [,1]        \n",
       "                  2.419803e-04\n",
       "pass_vs_block     1.534096e-05\n",
       "recipients        2.413437e-04\n",
       "sanitation_checks 3.180167e-03\n",
       "spamscore         9.798224e-04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad.desc(X,y, 0.0000454, 0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row></th><td> 0.0002196734</td></tr>\n",
       "\t<tr><th scope=row>pass_vs_block</th><td> 0.0002589650</td></tr>\n",
       "\t<tr><th scope=row>recipients</th><td>-0.0001098847</td></tr>\n",
       "\t<tr><th scope=row>sanitation_checks</th><td> 0.0014898290</td></tr>\n",
       "\t<tr><th scope=row>spamscore</th><td> 0.0045359688</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\t &  0.0002196734\\\\\n",
       "\tpass\\_vs\\_block &  0.0002589650\\\\\n",
       "\trecipients & -0.0001098847\\\\\n",
       "\tsanitation\\_checks &  0.0014898290\\\\\n",
       "\tspamscore &  0.0045359688\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  |  0.0002196734 | \n",
       "| pass_vs_block |  0.0002589650 | \n",
       "| recipients | -0.0001098847 | \n",
       "| sanitation_checks |  0.0014898290 | \n",
       "| spamscore |  0.0045359688 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                  [,1]         \n",
       "                   0.0002196734\n",
       "pass_vs_block      0.0002589650\n",
       "recipients        -0.0001098847\n",
       "sanitation_checks  0.0014898290\n",
       "spamscore          0.0045359688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad.desc(X,y, 0.000045, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>0.076957648046684</dd>\n",
       "\t<dt>pass_vs_block</dt>\n",
       "\t\t<dd>0.78801589965007</dd>\n",
       "\t<dt>recipients</dt>\n",
       "\t\t<dd>-0.0298598646539911</dd>\n",
       "\t<dt>sanitation_checks</dt>\n",
       "\t\t<dd>-0.00365906749674724</dd>\n",
       "\t<dt>spamscore</dt>\n",
       "\t\t<dd>0.00167206588197064</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 0.076957648046684\n",
       "\\item[pass\\textbackslash{}\\_vs\\textbackslash{}\\_block] 0.78801589965007\n",
       "\\item[recipients] -0.0298598646539911\n",
       "\\item[sanitation\\textbackslash{}\\_checks] -0.00365906749674724\n",
       "\\item[spamscore] 0.00167206588197064\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   0.076957648046684pass_vs_block\n",
       ":   0.78801589965007recipients\n",
       ":   -0.0298598646539911sanitation_checks\n",
       ":   -0.00365906749674724spamscore\n",
       ":   0.00167206588197064\n",
       "\n"
      ],
      "text/plain": [
       "      (Intercept)     pass_vs_block        recipients sanitation_checks \n",
       "      0.076957648       0.788015900      -0.029859865      -0.003659067 \n",
       "        spamscore \n",
       "      0.001672066 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm(percent_spam ~ pass_vs_block + recipients + sanitation_checks + spamscore, mail)$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By far $\\alpha \\approx 0.000045$ is the max alpha value I found. Values larger than this value will diverge.\n",
    "\n",
    "For this $\\alpha$ value, despite of the choice of $\\epsilon$, the predicted coefficients will be all quite small, which is similar with the result produced by lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
