{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPCS 53111 - Homework 5\n",
    "\n",
    "# Question 2\n",
    "\n",
    "#### NOTICE:  I think CNNLayer should be just a layer on a CNN network, An object called Convolutional Neural Network should be responsible for taking in multiple images as parameters, while CNNLayer is just an object that comprises a CNN. So CNNLayer each time takes in just one image to be processed, not N images. So I'm not following the structures given by homework description, in this perspective. \n",
    "\n",
    "Code for the last homework has been used to define Neuron and FCLayer, being used for Fully Connected layer object. \n",
    "\n",
    "This is modified version of last homework which is to define Neuron and Fully-connected layer object. CNNLayer follows it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### this is Neuron class. \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "  \n",
    "class Neuron:\n",
    "    # size of weights = num_nodes_last_layer*num_weight\n",
    "    def __init__(self, weights_shape, bias=1,alpha=0.5):\n",
    " \n",
    "        # self.label is the true label.\n",
    "        self.label = None\n",
    "        self.bias = bias\n",
    "        # input is a list such that each item is a pd.DataFrame\n",
    "        self.inputs = None\n",
    "        self.predicted_label = None\n",
    "        self.input_shape = None\n",
    "        self.error = None\n",
    "        self.true_label = None\n",
    "        # learning_rate\n",
    "        self.alpha = alpha\n",
    "        # same size as self.inputs\n",
    "        self.weights = None\n",
    "        # same size as self.weights\n",
    "        # should be passed back to previous layer        \n",
    "        self.back_propagation_delta = None\n",
    "        # size of weights is the same as size of a picture(1,M)\n",
    "        self.weights_shape = weights_shape        \n",
    "        # internal delta used to update weights\n",
    "        self.delta = None \n",
    "        \n",
    "        if self.weights is None:\n",
    "            self.weights = (np.random.random([self.weights_shape[0],\\\n",
    "                           self.weights_shape[1],self.weights_shape[2]])-.5)*0.000000000001\n",
    "                                                 \n",
    "    def neuron_delta(self):\n",
    "        # delta is a number type\n",
    "        self.delta = self.error*self.derivative()\n",
    "        \n",
    "    # notitce: call this function before update weights!!!\n",
    "    # returns a delta matrix that has then same size as self.inputs\n",
    "    def back_propagation(self):\n",
    "        self.neuron_delta()\n",
    "        self.back_propagation_delta = self.error*self.delta.sum()\n",
    "        return self.back_propagation_delta\n",
    " \n",
    "    def calculate_error(self,y):\n",
    "        self.true_label = y\n",
    "        self.error = self.true_label - self.predicted_label\n",
    "        \n",
    "    def g(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.input_shape = inputs.shape\n",
    "        self.predicted_label = self.logistic_regression(self.calculate_in())\n",
    "        self.output_shape = (self.input_shape[0],1)\n",
    "  \n",
    "        # should be of shape (inputs[0],1)\n",
    "        return self.predicted_label\n",
    "\n",
    "    # calculate \n",
    "    def calculate_in(self):                \n",
    "        # inputs[1] and weights[1] are DataFrame\n",
    "        return np.dot(self.inputs, self.weights).sum() + self.bias\n",
    "\n",
    "    # this is g'(in)\n",
    "    def derivative(self,epsilon = 0.001):   \n",
    "        return (self.logistic_regression(self.calculate_in()+epsilon) - \\\n",
    "                self.logistic_regression(self.calculate_in()-epsilon))/(2*epsilon)\n",
    "\n",
    "    # logistic function\n",
    "    def logistic_regression(self, z):\n",
    "        return 1 / (1 + np.exp(-z))        \n",
    "\n",
    "    # notice: don't call it until calling back_propagation !\n",
    "    def update_weights(self):\n",
    "        if self.delta is None:\n",
    "            print(\"Call self.neuron_delta() first to compute delta of this Neuron\")\n",
    "            return\n",
    "        self.weights = self.weights + self.alpha*self.delta*self.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each Layer object contains neuron_num numbers of neuron objects\n",
    "# this FCLayer is different from ANN.\n",
    "class FCLayer:\n",
    "    # neuron_num is the number of neurons within this layer\n",
    "    # weight_num is how many weights for each neuron\n",
    "    def __init__(self, alpha,neuron_num,weights_shape, true_label, bias=1):\n",
    "        self.alpha = alpha\n",
    "        self.true_label = true_label\n",
    "        # default bias = 1\n",
    "        self.bias = bias \n",
    "        # neurons \n",
    "        self.neurons = []\n",
    "        # how many neurons \n",
    "        self.neuron_num= neuron_num\n",
    "        # self.output is the output that feeds to the next layer\n",
    "        self.output = None\n",
    "        # contains deltas from all neurons in layer\n",
    "        # shape should be # of neurons+self.input.shape\n",
    "        self.layer_deltas  = []\n",
    "        # this is the final delta matrix passed back to previous CNNLayer\n",
    "        self.layer_delta  = []\n",
    "        self.weights_shape = weights_shape\n",
    "        \n",
    "        for i in range(neuron_num):\n",
    "            self.neurons.append(Neuron(self.weights_shape,self.bias))\n",
    "\n",
    "    # calculate the output for each neuron\n",
    "    # store it in self.output\n",
    "    def feed_forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.output = []\n",
    "        for neuron in self.neurons:\n",
    "            self.output.append(neuron.g(inputs))\n",
    "        self.output = np.array(self.output)\n",
    "       \n",
    "    # return output to next layer\n",
    "    # size should be the same self.inputs\n",
    "    def back_propagation(self):\n",
    "        # must call calculate_error in each neuron first \n",
    "        # then neuron_delta can be called.\n",
    "        # assign true_label to each neuron in layer\n",
    "        for i in range(len(self.true_label)):\n",
    "            self.neurons[i].calculate_error(self.true_label[i])\n",
    "        self.layer_deltas = []\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            self.layer_deltas.append(neuron.back_propagation())\n",
    "  \n",
    "        self.layer_deltas = np.array(self.layer_deltas)      \n",
    "\n",
    "        # passed to previous layer\n",
    "        self.layer_delta = np.zeros(self.inputs.shape)        \n",
    "        for i in range(len(self.layer_deltas)):\n",
    "            self.layer_delta = self.layer_delta + self.layer_deltas[i]*self.neurons[i].weights\n",
    "            \n",
    "        return self.layer_delta\n",
    "    # call after calling back_propagation()\n",
    "    def update_weights(self):\n",
    "        for i in range(len(self.neurons)):\n",
    "            self.neurons[i].update_weights()\n",
    "            \n",
    "    def set_true_label(self,true_label):\n",
    "        self.true_label=true_label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is CNNLayer Object\n",
    "# the following CNNLayer class is new \n",
    "class CNNLayer:\n",
    "    \n",
    "    # notice: order of filter shape should be (depth,height,width)\n",
    "    #          order of filter shape should be (height,width,depth)  \n",
    "    def __init__(self,n=32,filter_shape=(4,5,5),activation=\"relu\",stride=2,alpha=100):\n",
    "        self.bias = 1\n",
    "        self.input = input\n",
    "        self.filter_shape = tuple([n]+list(filter_shape))\n",
    "        self.alpha = alpha\n",
    "        self.stride = stride\n",
    "        # contains filter arrays\n",
    "        self.filters = None\n",
    "        # each image's shape\n",
    "        self.output_shape = None\n",
    "        # layer_delta is the delta matrix passed back to previous layer\n",
    "        self.layer_delta = None\n",
    "        # this is the delta matrix from next layer\n",
    "        self.next_layer_delta =None\n",
    "        # self.output is the output by forward computation going to next layer\n",
    "        self.output = None\n",
    "        # number of filters\n",
    "        self.filter_layer = n\n",
    "        # input with 0 being padded\n",
    "        self.padded_input = None\n",
    "        # bias is a array of size n, same as the number of filters\n",
    "        # for now just randomly choose from 1 and 0\n",
    "        self.bias=np.random.choice([0,1],n)\n",
    "        \n",
    "        if activation == \"no_act\":\n",
    "            self.activation = self._noact\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = self._ReLU   \n",
    "            \n",
    "        # notice that the filters are defines as [depth,width,height] rather\n",
    "        # than [width,height,depth]\n",
    "        self.filters = np.random.choice([-1,0,1],n*filter_shape[0]*filter_shape[1]*\\\n",
    "                filter_shape[2]).reshape([n,filter_shape[0],filter_shape[1],\\\n",
    "                                         filter_shape[2]])\n",
    "                         \n",
    "    # Notice: the padding functionality assumes that the input images\n",
    "    # are square size, which means width and height are of same size. \n",
    "    # Be lazy here.   \n",
    "    def _padding(self,pad =0):\n",
    "        \n",
    "        output_size=int(np.ceil((self.input.shape[2]-self.filters[0].shape[2])/self.stride+1))       \n",
    "        # output size for whole set of image\n",
    "        self.output_shape=(self.filter_layer,output_size,output_size)\n",
    "        # output size for the whole set of images\n",
    "        self.output = np.zeros([self.filter_layer,output_size,output_size])\n",
    "        \n",
    "        # calculating number of rows/cols to pad. \n",
    "        # padding_length = 1 means add 1 row and column to surface of self.input\n",
    "        self.padding_length = int(np.ceil((self.output_shape[1]-1)*self.stride - \\\n",
    "                          self.input.shape[2] + self.filter_shape[2]))\n",
    "\n",
    "        # create a new image with 0 being padded\n",
    "        if self.padding_length > 0:\n",
    "            new_images = []\n",
    "    \n",
    "            for each_depth in self.input:\n",
    "                new_image = np.insert(each_depth,len(each_depth),\\\n",
    "                                           np.zeros(each_depth.shape[0]),axis=1)\n",
    "                new_image  = np.insert(new_image,len(new_image),\\\n",
    "                                       np.zeros(new_image.shape[1]),axis=0)\n",
    "                \n",
    "                for i in range(self.padding_length-1):\n",
    "                    new_image = np.insert(new_image,len(new_image),\\\n",
    "                                           np.zeros(new_image.shape[0]),axis=1)\n",
    "                    new_image  = np.insert(new_image,len(new_image),\\\n",
    "                                       np.zeros(new_image.shape[1]),axis=0)\n",
    "                \n",
    "                new_images.append(new_image)               \n",
    "            self.padded_input = np.array(new_images)         \n",
    "        else: \n",
    "            self.padded_input = self.input\n",
    "        # END of padding\n",
    "        # create output array\n",
    "        self.output = np.zeros([self.filter_layer,(self.padded_input.shape[1]-\\\n",
    "                       self.filters[0].shape[1])/self.stride+1,\\\n",
    "                       (self.padded_input.shape[1]-self.filters[0].shape[1])/self.stride+1])\n",
    "                        \n",
    "    def forward_step(self,X):        \n",
    "        if X.shape[0] != self.filter_shape[1]:\n",
    "            print(\"filter depth does not match image depth!\")\n",
    "        \n",
    "        # input image        \n",
    "        self.input = X     \n",
    "        self._padding(self.input)\n",
    "        \n",
    "        # forward computation\n",
    "        for k in range(self.output_shape[0]):\n",
    "            current_filter = self.filters[k]\n",
    "\n",
    "            for i in range(self.output_shape[1]):\n",
    "                for j in range(self.output_shape[2]):\n",
    "                    self.output[k,i,j] =  self.activation(1+(current_filter*\\\n",
    "                            self.padded_input[:,self.stride*i:self.stride*i+\\\n",
    "                          current_filter.shape[1],self.stride*j:self.stride*j+\\\n",
    "                            current_filter.shape[2]]).sum())                      \n",
    "        return self.output\n",
    "    # by default, there should be random filters assigned for debugging purpose \n",
    "    # so even if you don't set filters, self.filters will still be there\n",
    "    \n",
    "    def set_filters(self,filters):   \n",
    "        if filters.shape != self.filter_shape:\n",
    "            print(\"filters must be numpy array with shape as\",self.filter_shape)        \n",
    "        else:\n",
    "            self.filters = filters\n",
    "            \n",
    "    # set bias\n",
    "    def set_bias(self,bias):   \n",
    "        if bias.shape != self.bias.shape:\n",
    "            print(\"filters must be numpy array with shape as\",self.bias.shape)        \n",
    "        else:\n",
    "            self.bias = bias\n",
    "    \n",
    "    # print filters\n",
    "    def print(self):\n",
    "        print(\"Filter shape:\",self.filter_shape)\n",
    "        \n",
    "        for i in range(len(self.filters)):\n",
    "            print(\"Filter layer\",i+1,\":\")\n",
    "            print(self.filters[i],\"\\n\")\n",
    "\n",
    "    def _noact(self,x):\n",
    "        return x                \n",
    "    def _ReLU(self,x):\n",
    "        return max(0,x)\n",
    "        \n",
    "    # this is g'(in)\n",
    "    def derivative(self,f,data,epsilon = 0.001):\n",
    "        return (f(data+epsilon) - f(data-epsilon))/(2*epsilon)                 \n",
    "    \n",
    "    def backward_step_from_fc(self,next_layer_delta):\n",
    "        # assert\n",
    "        if next_layer_delta.shape != self.output.shape:\n",
    "            print(\"Delta Matrix received from next layer does not match current output!\") \n",
    "            return \n",
    "        \n",
    "        self.next_layer_delta = next_layer_delta\n",
    "        # compute g prime\n",
    "        f_add_h = np.zeros(self.output.shape)\n",
    "        f_minus_h = np.zeros(self.output.shape)\n",
    "        h = 0.001\n",
    "        \n",
    "        # f_add_h f_minus_h \n",
    "        for k in range(self.output_shape[0]):\n",
    "            current_filter = self.filters[k]\n",
    "\n",
    "            for i in range(self.output_shape[1]):\n",
    "                for j in range(self.output_shape[2]):\n",
    "                    f_add_h[k,i,j] =  self.activation(1+(current_filter*\\\n",
    "                            self.padded_input[:,self.stride*i:self.stride*i+\\\n",
    "                          current_filter.shape[1],self.stride*j:self.stride*j+\\\n",
    "                            current_filter.shape[2]]).sum()+h) \n",
    "                    \n",
    "                    f_minus_h[k,i,j] =  self.activation(1+(current_filter*\\\n",
    "                            self.padded_input[:,self.stride*i:self.stride*i+\\\n",
    "                          current_filter.shape[1],self.stride*j:self.stride*j+\\\n",
    "                            current_filter.shape[2]]).sum()-h) \n",
    "        # g_prime\n",
    "        self.g_prime = (f_add_h - f_minus_h)/2*h\n",
    "        self.delta_j = self.next_layer_delta*self.g_prime\n",
    "      \n",
    "        # compute delta matrix passed to previous layer\n",
    "        self.layer_delta = np.zeros(self.delta_j.shape[0])\n",
    "        \n",
    "        for k in range(len(self.filters)):\n",
    "            self.layer_delta[k] = (self.delta_j*self.filters[k]).sum()\n",
    "        return self.layer_delta\n",
    " \n",
    "        \n",
    "    def backward_step_from_cnn(self,next_layer_delta):      \n",
    "        \n",
    "        self.next_layer_delta = next_layer_delta\n",
    "        self.new_layer_delta = np.zeros(self.filter_shape)\n",
    "        \n",
    "        for i in range(len(self.next_layer_delta)):\n",
    "           self.new_layer_delta[i] = self.next_layer_delta[i]\n",
    "\n",
    "        self.next_layer_delta = self.new_layer_delta\n",
    "        \n",
    "        self.new_output =np.zeros(self.output.shape[0])\n",
    "        for i in self.output:\n",
    "            self.new_output = i.sum()\n",
    "\n",
    "        self.output = self.new_output\n",
    "            \n",
    "    def update_weights_from_fc(self):\n",
    "        \n",
    "        self.new_filters = np.zeros(self.filters.shape)        \n",
    "        for i in range(len(self.filters)):\n",
    "            self.new_filters = self.filters + self.alpha*self.next_layer_delta[i]*self.output\n",
    "        self.filters = self.new_filters\n",
    "\n",
    "    def update_weights_from_cnn(self):\n",
    "        self.filters = self.filters + self.alpha*self.next_layer_delta*self.output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### quick test on the shapes of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: an image with width:28, height:28, depth:4, stride=2, filter_shape: 5*5*4, number of filters: 32\n",
      "Shape of the filters of the first CNN Layer:\n",
      "Order of dimension: \n",
      "number of filters, depth of image, width of image, height of image\n",
      "(32, 4, 5, 5)\n",
      "Shape of the output image from the first CNN Layer:\n",
      "(32, 13, 13)\n",
      "Shape of the filters of the second CNN Layer:\n",
      "(32, 4, 5, 5)\n",
      "Shape of the output image from the second CNN Layer:\n",
      "(32, 13, 13)\n",
      "Shape of the output from the FC Layer:\n",
      "(10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:73: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# input: an image with width:28, height:28, depth:4\n",
    "# number of filters is defaulted to be 32, each of which is 5*5*4 size. \n",
    "a=np.random.randint(100,size=[4,28,28])\n",
    "\n",
    "# initate two CNN Layers\n",
    "x = CNNLayer(n=32,filter_shape=(4,5,5),activation=\"no_act\",stride=2,alpha=1)        \n",
    "x.forward_step(a)\n",
    "\n",
    "# print out output shape of the the first CNN Layer \n",
    "print(\"input: an image with width:28, height:28, depth:4, stride=2, filter_shape: 5*5*4, number of filters: 32\")\n",
    "print(\"Shape of the filters of the first CNN Layer:\")\n",
    "print(\"Order of dimension: \")\n",
    "print(\"number of filters, depth of image, width of image, height of image\")\n",
    "print(x.filters.shape)\n",
    "print(\"Shape of the output image from the first CNN Layer:\")\n",
    "print(x.output.shape)\n",
    "y = CNNLayer(n=32,filter_shape=(x.output_shape[0],5,5),activation=\"relu\",stride=2,alpha=1)\n",
    "\n",
    "# use the first layer's output image as input for the second CNN Layer\n",
    "y.forward_step(x.output)\n",
    "print(\"Shape of the filters of the second CNN Layer:\")\n",
    "print(x.filters.shape)\n",
    "print(\"Shape of the output image from the second CNN Layer:\")\n",
    "print(x.output.shape)\n",
    "\n",
    "# Initiate a FC layer \n",
    "n = FCLayer(0.1,10,y.output_shape,np.array([1,1,0,0,0,1,1,1,0,0]))\n",
    "\n",
    "# predict an image \n",
    "n.feed_forward(y.output)\n",
    "print(\"Shape of the output from the FC Layer:\")\n",
    "print(n.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 \n",
    "\n",
    "###### I don't want to build another class object based on CNNLayer object. After suffering so much on Question 2, it really kills any desire and willingness to tweak it any furthur. Running out of time, sorry about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 784)\n",
      "(200, 10)\n",
      "300 images, each of which has 10 labels corresponding to 10 neurons in the FC Layer\n"
     ]
    }
   ],
   "source": [
    "# Question 4 - Preparing training images\n",
    "\n",
    "raw=pd.read_csv(\"C:\\\\Users\\\\zheny\\\\Downloads\\\\train.csv\")\n",
    "\n",
    "# create a sample of images that includes images from 0 to 9 \n",
    "sample_size = 20 # 20 images for each handwritting \n",
    "train_dataset = pd.DataFrame([])\n",
    "for i in range(10):\n",
    "    train_dataset = pd.concat([train_dataset,raw[raw.iloc[:,0] == i].sample(sample_size)])\n",
    "\n",
    "labels = train_dataset.iloc[:,0]\n",
    "y = [] \n",
    "\n",
    "for i in labels:\n",
    "    label_column = [0 for i in range(10)]\n",
    "    label_column[i] = 1\n",
    "    y.append(label_column)\n",
    "    \n",
    "# y is the label columns, must be numpy array\n",
    "y = np.array(y)\n",
    "# X is training images, must be numpy array\n",
    "X = np.array(train_dataset.iloc[:,1:])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(\"300 images, each of which has 10 labels corresponding to 10 neurons in the FC Layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:73: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 Iteration the 1 Image\n",
      "[ 0.73105865  0.73105818  0.73105893  0.73105722  0.73105955  0.73105953\n",
      "  0.73105934  0.73105887  0.73105837  0.73105856]\n",
      "the 1 Iteration the 2 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 3 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 4 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 5 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 6 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 7 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 8 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 9 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 10 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 11 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 12 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 13 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 14 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 15 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 16 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 17 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 18 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 19 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 20 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 21 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 22 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 23 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 24 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 25 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 26 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 27 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 28 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 29 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 30 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 31 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 32 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 33 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 34 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 35 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 36 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 37 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 38 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 39 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 40 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 41 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 42 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 43 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 44 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 45 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 46 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 47 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 48 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 49 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 50 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 51 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 52 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 53 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 54 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 55 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 56 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 57 Image\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "the 1 Iteration the 58 Image\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bbe711b3ea06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[1;31m# forward computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mfirst_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0msecond_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mfc_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7702752c3696>\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_filter\u001b[0m\u001b[1;33m*\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadded_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m                          \u001b[0mcurrent_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m                            \u001b[0mcurrent_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[1;31m# by default, there should be random filters assigned for debugging purpose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Question 4 - training\n",
    "\n",
    "# Notice: Please be advised that the whole implementation is very sensitive to the \n",
    "# weights of the FC Layer, if it's initialized too big/small then the whole back propagation \n",
    "# will never converge because its derivative, computed as (f(x+h) - f(x-h))/2h, will always \n",
    "# be 0, which in turn causes the delta passed back to previous layer be 0.     \n",
    "\n",
    "# configuration: \n",
    "# number of filters\n",
    "n = 32 \n",
    "# filters size: depth 1, heigth&weight 5\n",
    "filter_size =(1,5,5)\n",
    "# stride \n",
    "stride = 2\n",
    "# number of node in FC \n",
    "node = 10\n",
    "# learning rate = 1\n",
    "alpha = 0.0001\n",
    "# iteration \n",
    "t = 10\n",
    "# first image\n",
    "image=X[0].reshape([1,28,28])\n",
    "\n",
    "# first CNNLayer\n",
    "first_layer = CNNLayer(n = n, filter_shape= filter_size,stride=stride,alpha = alpha)    \n",
    "first_layer.forward_step(image)  \n",
    "\n",
    "# second CNNLayer\n",
    "second_layer = CNNLayer(filter_shape=(first_layer.output_shape[0],5,5),alpha = alpha)\n",
    "second_layer.forward_step(first_layer.output)\n",
    "\n",
    "# FC Layer\n",
    "fc_layer = FCLayer(alpha=alpha,neuron_num=node,weights_shape = second_layer.output_shape,true_label=y[0])\n",
    "\n",
    "#before = y.filters.sum()\n",
    "before = fc_layer.neurons[0].weights[0].sum() \n",
    "\n",
    "for i in range(t-1):   \n",
    "    for j in range(len(X)):\n",
    "        \n",
    "        print(\"the\", i+1,\"Iteration the\", j+1,\"Image\")\n",
    "        \n",
    "        image=X[j].reshape([1,28,28])\n",
    "        fc_layer.set_true_label(y[j])\n",
    "        \n",
    "        # forward computation      \n",
    "        first_layer.forward_step(image)\n",
    "        second_layer.forward_step(first_layer.output)\n",
    "        fc_layer.feed_forward(second_layer.output)\n",
    "        print(fc_layer.output)\n",
    "        # backward propagation\n",
    "        fc_layer.back_propagation()\n",
    "        second_layer.backward_step_from_fc(fc_layer.layer_delta)\n",
    "        first_layer.backward_step_from_cnn(second_layer.layer_delta)\n",
    "\n",
    "        # update weights\n",
    "        first_layer.update_weights_from_cnn()\n",
    "        second_layer.update_weights_from_fc()\n",
    "        fc_layer.update_weights()\n",
    "\n",
    "#print( y.filters.sum()-before)\n",
    "print(fc_layer.neurons[0].weights[0].sum() -before)\n",
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 784)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "# Question 4 - Creating Testing Dataset of Images \n",
    "\n",
    "# create a sample of images that includes images from 0 to 9 \n",
    "sample_size = 5 #  number of images for each handwritting \n",
    "\n",
    "test_dataset = pd.DataFrame([])\n",
    "for i in range(10):\n",
    "    test_dataset = pd.concat([test_dataset,raw[raw.iloc[:,0] == i].sample(sample_size)])\n",
    "\n",
    "# y is the label columns, must be numpy array\n",
    "test_y = np.array(test_dataset.iloc[:,0])\n",
    "# X is test images, must be numpy array\n",
    "test_X = np.array(test_dataset.iloc[:,1:])\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:73: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    predict_0  predict_1  predict_2  predict_3  predict_4  predict_5  \\\n",
      "0         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "1         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "2         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "3         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "4         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "5         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "6         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "7         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "8         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "9         1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "10        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "11        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "12        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "13        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "14        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "15        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "16        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "17        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "18        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "19        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "20        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "21        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "22        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "23        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "24        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "25        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "26        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "27        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "28        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "29        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "30        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "31        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "32        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "33        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "34        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "35        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "36        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "37        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "38        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "39        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "40        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "41        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "42        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "43        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "44        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "45        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "46        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "47        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "48        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "49        1.0        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "    predict_6  predict_7  predict_8  predict_9  True Label  \n",
      "0         0.0        0.0        0.0        0.0           0  \n",
      "1         0.0        0.0        0.0        0.0           0  \n",
      "2         0.0        0.0        0.0        0.0           0  \n",
      "3         0.0        0.0        0.0        0.0           0  \n",
      "4         0.0        0.0        0.0        0.0           0  \n",
      "5         0.0        0.0        0.0        0.0           1  \n",
      "6         0.0        0.0        0.0        0.0           1  \n",
      "7         0.0        0.0        0.0        0.0           1  \n",
      "8         0.0        0.0        0.0        0.0           1  \n",
      "9         0.0        0.0        0.0        0.0           1  \n",
      "10        0.0        0.0        0.0        0.0           2  \n",
      "11        0.0        0.0        0.0        0.0           2  \n",
      "12        0.0        0.0        0.0        0.0           2  \n",
      "13        0.0        0.0        0.0        0.0           2  \n",
      "14        0.0        0.0        0.0        0.0           2  \n",
      "15        0.0        0.0        0.0        0.0           3  \n",
      "16        0.0        0.0        0.0        0.0           3  \n",
      "17        0.0        0.0        0.0        0.0           3  \n",
      "18        0.0        0.0        0.0        0.0           3  \n",
      "19        0.0        0.0        0.0        0.0           3  \n",
      "20        0.0        0.0        0.0        0.0           4  \n",
      "21        0.0        0.0        0.0        0.0           4  \n",
      "22        0.0        0.0        0.0        0.0           4  \n",
      "23        0.0        0.0        0.0        0.0           4  \n",
      "24        0.0        0.0        0.0        0.0           4  \n",
      "25        0.0        0.0        0.0        0.0           5  \n",
      "26        0.0        0.0        0.0        0.0           5  \n",
      "27        0.0        0.0        0.0        0.0           5  \n",
      "28        0.0        0.0        0.0        0.0           5  \n",
      "29        0.0        0.0        0.0        0.0           5  \n",
      "30        0.0        0.0        0.0        0.0           6  \n",
      "31        0.0        0.0        0.0        0.0           6  \n",
      "32        0.0        0.0        0.0        0.0           6  \n",
      "33        0.0        0.0        0.0        0.0           6  \n",
      "34        0.0        0.0        0.0        0.0           6  \n",
      "35        0.0        0.0        0.0        0.0           7  \n",
      "36        0.0        0.0        0.0        0.0           7  \n",
      "37        0.0        0.0        0.0        0.0           7  \n",
      "38        0.0        0.0        0.0        0.0           7  \n",
      "39        0.0        0.0        0.0        0.0           7  \n",
      "40        0.0        0.0        0.0        0.0           8  \n",
      "41        0.0        0.0        0.0        0.0           8  \n",
      "42        0.0        0.0        0.0        0.0           8  \n",
      "43        0.0        0.0        0.0        0.0           8  \n",
      "44        0.0        0.0        0.0        0.0           8  \n",
      "45        0.0        0.0        0.0        0.0           9  \n",
      "46        0.0        0.0        0.0        0.0           9  \n",
      "47        0.0        0.0        0.0        0.0           9  \n",
      "48        0.0        0.0        0.0        0.0           9  \n",
      "49        0.0        0.0        0.0        0.0           9  \n"
     ]
    }
   ],
   "source": [
    "# Question - Predicting \n",
    "\n",
    "result = []\n",
    "\n",
    "for j in range(len(test_X)):\n",
    "    \n",
    "    image=test_X[j].reshape([1,28,28])\n",
    "    \n",
    "    # forward computation      \n",
    "    first_layer.forward_step(image)\n",
    "    second_layer.forward_step(first_layer.output)\n",
    "    fc_layer.feed_forward(second_layer.output)\n",
    "    \n",
    "    # compiling predictions\n",
    "    image_prediction = list(fc_layer.output)\n",
    "    image_prediction.append(test_y[j])\n",
    "    result.append(image_prediction)\n",
    "\n",
    "index = ['0','1','2','3','4','5','6','7','8','9']\n",
    "col_name = [\"predict_\" + i for i in index]\n",
    "col_name.append(\"True Label\")\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "result.columns = col_name\n",
    "\n",
    "# see discussion below\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### discusion: \n",
    "\n",
    "Above is the result, first ten columns are if the image is predicted to be 0~9. last column is true labels. As you could see, it does not work totally. Something is wrong with the back propagation. If you take a look at the process of propagation, during the first iteration, input image is 0, the final output quickly changes from \n",
    "\n",
    "[ 0.73105865  0.73105818  0.73105893  0.73105722  0.73105955  0.73105953\n",
    "  0.73105934  0.73105887  0.73105837  0.73105856]\n",
    "\n",
    "to\n",
    "\n",
    "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "\n",
    "Then it stays the same for the rest of iteration. I suspect the derivative function does not work well when logistic function gives 1 or 0. The derivative of logistic function does not change at all given a small epsilon. So the derivative function is dead somehow, causing the back propagation process of FC Layer always gives delta as 0. Not sure how to solve it. This is the best I can do.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 4\n",
    "\n",
    "My result is doom.. something must be wrong, the accuracy is just like taking a random guess from 10 candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a5c21f09bd7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Setting up configurations:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cifar10\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def configuration():\n",
    "    # Setting up configurations:\n",
    "    cifar10.maybe_download_and_extract()\n",
    "\n",
    "    global alpha, image_size,thread_no, lable_numbers, k_size, X, y, weights,biases\n",
    "    alpha = 0.001\n",
    "    image_size = 32\n",
    "    depth = 3\n",
    "    lable_numbers = 10\n",
    "    k_size = 2\n",
    "    X = tf.placeholder(tf.float32, shape=[None, image_size, image_size, depth], name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, lable_numbers], name='y')\n",
    "\n",
    "    # initiate weights and biases according to homework specification\n",
    "    weights = []\n",
    "    biases= []\n",
    "    weights.append(tf.Variable(tf.random_normal([3, 3, 3, 64])))\n",
    "    weights.append(tf.Variable(tf.random_normal([3, 3, 64, 64])))\n",
    "    weights.append(tf.Variable(tf.random_normal([3, 3, 64, 128])))\n",
    "    weights.append(tf.Variable(tf.random_normal([3, 3, 128, 128])))\n",
    "    weights.append(tf.Variable(tf.random_normal([3, 3, 128, 256])))\n",
    "    weights.append(tf.Variable(tf.random_normal([4*4*256, 256])))\n",
    "\n",
    "    # for final fc layer\n",
    "    weights.append(tf.random_normal([256, lable_numbers]))\n",
    "    \n",
    "    biases.append(tf.Variable(tf.random_normal([64])))\n",
    "    biases.append(tf.Variable(tf.random_normal([64])))\n",
    "    biases.append(tf.Variable(tf.random_normal([128])))\n",
    "    biases.append(tf.Variable(tf.random_normal([128])))\n",
    "    biases.append(tf.Variable(tf.random_normal([256])))\n",
    "    biases.append(tf.Variable(tf.random_normal([256])))\n",
    "\n",
    "    # for final fc layer\n",
    "    biases.append(tf.Variable(tf.random_normal([lable_numbers])))\n",
    "\n",
    "    global final_layer,y_hat,cost,optimizer,initial_global_var,saver,\\\n",
    "           accurate_prediction,accuracy,number_accurate_prediction\n",
    "\n",
    "    # build a CNN: \n",
    "    final_layer, y_hat = build_cnn(X, weights, biases)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_layer, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(cost)\n",
    "\n",
    "    # initialization\n",
    "    initial_global_var = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # model evaluator \n",
    "    accurate_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(accurate_prediction, tf.float32))\n",
    "    number_accurate_prediction = tf.reduce_sum(tf.cast(accurate_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# make some aux function to call functions within tensorflow class \n",
    "def pool_layer(x, k_size=2):\n",
    "    \n",
    "    # print(tf.nn.max_pool(x,ksize=[1,k_size,k_size,1],strides=[1,k_size,k_size,1]))\n",
    "    return tf.nn.max_pool(x,ksize=[1,k_size,k_size,1],strides=[1,k_size,k_size,1],padding='SAME')\n",
    "\n",
    "def conv_layer(x, weights, bias, strides=1):\n",
    "    # processing input array\n",
    "    x = tf.nn.conv2d(x, weights, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    # print(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def fc_layer(input,weight,bias,size=[-1, 4*4*256]):\n",
    "    # fully connected layer\n",
    "    layer = tf.reshape(input, size)\n",
    "    layer = tf.add(tf.matmul(layer, weight), bias)\n",
    "    return tf.nn.relu(layer)\n",
    "\n",
    "# define a CNN model\n",
    "# notice: this is like a hard code function that only works for\n",
    "#         the specification of the 10 layers CNN specified by\n",
    "#         the homework requirement.\n",
    "# returns final predict and layer for debug\n",
    "                   \n",
    "def build_cnn(x, weights, biases):\n",
    "\n",
    "    # according to the specification of the question\n",
    "\n",
    "    conv1 = conv_layer(X, weights[0], biases[0])\n",
    "    conv2 = conv_layer(conv1, weights[1], biases[1])\n",
    "    \n",
    "    pool1 = pool_layer(conv2)\n",
    "    conv3 = conv_layer(pool1, weights[2], biases[2])\n",
    "    conv4 = conv_layer(conv3, weights[3], biases[3])\n",
    "    pool2 = pool_layer(conv4)\n",
    "    conv5 = conv_layer(pool2, weights[4], biases[4])\n",
    "    pool3 = pool_layer(conv5)\n",
    "    \n",
    "    fc1 = fc_layer(pool3,weights[5], biases[5])\n",
    "\n",
    "    fc2 = tf.add(tf.matmul(fc1,weights[6]),biases[6])\n",
    "    result = tf.nn.softmax(fc2)\n",
    "\n",
    "    return fc2,result\n",
    "\n",
    "\n",
    "# number of batch to train on \n",
    "def train(batch = 5,iteration = 100):\n",
    "\n",
    "    number_of_batch = 100\n",
    "    batch_per_iter = int(50000/number_of_batch)\n",
    "    train_images, train_cls, train_labels = cifar10.load_training_data()\n",
    "    \n",
    "    with tf.Session() as thread:\n",
    "        thread.run(initial_global_var)\n",
    "        \n",
    "        for i in range(batch):\n",
    "            print(\"Now: \"+str(i)+\" iteration\")\n",
    "\n",
    "            step_size = i%batch_per_iter \n",
    "\n",
    "            current_x, current_y = train_images[step_size*number_of_batch:(step_size + 1)\\\n",
    "                        *number_of_batch],train_labels[step_size*number_of_batch:(step_size+1)*number_of_batch]\n",
    "            \n",
    "            thread.run(optimizer, feed_dict={X: current_x, y: current_y})\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            \n",
    "            for j in range(iteration):\n",
    "                current_loss, current_accuracy = thread.run([cost,accuracy],\\\n",
    "                        feed_dict={X:train_images[j*number_of_batch:(j+1)*number_of_batch],\\\n",
    "                        y:train_labels[j*number_of_batch:(j+1)*number_of_batch]})\n",
    "\n",
    "                total_loss=total_loss+current_loss\n",
    "                total_accuracy = total_accuracy+current_accuracy\n",
    "\n",
    "            total_loss = total_loss/iteration\n",
    "            total_accuracy = total_accuracy/iteration\n",
    "\n",
    "            print(\"Batch \"+ str(i)  + \", current raining Accuracy= \", total_accuracy)\n",
    "        saver.save(thread, os.path.join(os.getcwd()))\n",
    "          \n",
    "    print(\"training done\\n\")\n",
    "\n",
    "\n",
    "# to speed up, the number of images to be tested is down to 10\n",
    "def test(test_image_num = 10):\n",
    "\n",
    "    test_images, test_cls, test_labels = cifar10.load_test_data()\n",
    "    number_of_batch = 100\n",
    "    \n",
    "    with tf.Session() as thread:\n",
    "\n",
    "        model = tf.train.import_meta_graph(os.path.join(os.getcwd(),\"model.meta\"))\n",
    "        model.restore(thread, os.path.join(os.getcwd(),\"model\"))\n",
    "\n",
    "        accuracy = []\n",
    "        \n",
    "        for i in range(test_image_num):\n",
    "        \n",
    "            accuracy.append(thread.run(number_accurate_prediction,feed_dict={X:test_images[i*number_of_batch:(i+1)*number_of_batch],\\\n",
    "                            y:test_labels[i*number_of_batch:(i+1)*number_of_batch]}))\n",
    "\n",
    "        print(\"prediction accuracy of the first testing images: \", np.array(accuracy)/100.0)\n",
    "        \n",
    "\n",
    "def main(args):\n",
    "\n",
    "    if args[1] ==\"train\":\n",
    "        print(\"To illastrate the whole program, iteration and batch size has been reduce lower enough to speed up the process time.\")\n",
    "        configuration()                    \n",
    "        train()\n",
    "        \n",
    "    elif args[1] == \"test\":\n",
    "        test()\n",
    "\n",
    "    else:\n",
    "        print(\"This program takes 1 argument: hw5-tf.py -test/-train\")\n",
    "        return\n",
    "                            \n",
    "    return\n",
    "                            \n",
    "main(['hw.py','train'])\n",
    "main(['hw.py','test'])\n",
    "#main(sys.argv)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
