{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPCS 53111 - homework 4\n",
    "\n",
    "## Name: Zhenyang Lu\n",
    "\n",
    "# question 2 \n",
    "\n",
    "Notice: The way I did this homework is not designing it first then implementing it. It's rather the opposite like reverse engineering. I honestly don't know how to design it from theory. So below is best I can do. \n",
    "\n",
    "## Design: \n",
    "\n",
    "Activation function: Linear \n",
    "\n",
    "Three input variables: \n",
    "\n",
    "$X_1$        \n",
    "\n",
    "$X_2$\n",
    "\n",
    "$X_1*X_2$\n",
    "\n",
    "Hidden Layer: \n",
    "\n",
    "Neuron 1: take $X_1$ with weights -1, $X_1*X_2$ with weights 1, bias = 0.19\n",
    "\n",
    "Neuron 2: take $X_2$ with weights -1, $X_1*X_2$ with weights 1, bias = 0.1\n",
    "\n",
    "Final Output: \n",
    "\n",
    "take 0.41 weight from Neuron 1, 0.52 weight from Neuron 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "when considering possibility of incorrect setting. The two parts of the original formular needs to be modified to include possibility of error :\n",
    "\n",
    "$(y^i*log(h(x^i))$ should be becomes $y^i*(log(h(x^i)(1-\\delta) + (1-h(x^i))*\\delta)) $\n",
    "\n",
    "$(1-y^i)log(1-h(x^i)))$ should be like $(1-y^i)*log((1-h(x^i))(1-\\delta)+h(x^i)*\\delta)$\n",
    "\n",
    "So combining together the two scenarios, we have  \n",
    "\n",
    "$l(\\theta) = \\sum^m_{i=1} = y^i*log(h(x^i)(1-\\delta) + (1-h(x^i))*\\delta) + (1-y^i)*log((1-h(x^i))(1-\\delta)+h(x^i)*\\delta)$\n",
    "\n",
    "which should be the same as the original formula that Andrew Ng derives if $\\delta = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 \n",
    "\n",
    "I'm very confused by this question. It seems like very straightforward to me, what the neuron network tries to predict (also, why the author calls it a back propagation network, rather than Neuro network, or there is some difference between the two). It's just 80% of time it predicts 1, and 20% it predicts 0, so if it's equally weighted and independent with each other, its expected prediction is 0.8.\n",
    "\n",
    "But following the hint given, which is totally another direction of thinking, it's also very straightforward: \n",
    "\n",
    "Define X to be the true label for the only example. Cost function: \n",
    "\n",
    "$J(\\theta) = \\frac{1}{2} [80*(1-X)^2 + 20(0-X)^2] $, if taken derivative with respect to X (I don't think we should take derivative with respect to the predicted value h(x) because it's either 0 or 1. The only independent variable is X.), we get 100X-80, so X is still 0.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "raw = pd.read_csv(\"C:\\\\Users\\\\zheny\\\\Downloads\\\\train.csv\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "For this question, there are three objects: Neuron, Layer(contains Neurons) and ANN( artificial network, which contains Layers). Following is Neuron, variable definitions and method functioning is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "class Neuron:\n",
    "    # size of weights = num_nodes_last_layer*num_weight\n",
    "    # b is uniform dist parameter Xavier initialization,  \n",
    "    # b = 6/2*(number of nodes in each layer)**(1/2)\n",
    "    \n",
    "    def __init__(self, num_weight, b,bias=1):\n",
    "        \n",
    "        self.bias = bias\n",
    "        # input is a list such that each item is a pd.DataFrame\n",
    "        self.input = None\n",
    "        # contains g(W*a)\n",
    "        self.output = None\n",
    "        # input dataframe size\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        # weights vector\n",
    "        self.weights = None \n",
    "        # weights is a list such that each item is a pd.DataFrame\n",
    "        # same size as self.inputs\n",
    "        \n",
    "        # size of weights is the same as size of a picture(1,M)\n",
    "        self.num_weight = num_weight\n",
    "        # this is g'(W*a)       \n",
    "        self.delta = None \n",
    "        \n",
    "        if self.weights is None:\n",
    "            self.weights = pd.DataFrame(np.random.uniform(-b,b,self.num_weight))\n",
    "        \n",
    "    # calculate neuron's delta when back propagation for outout layer\n",
    "    def output_neuron_delta(self,error):\n",
    "        # delta is a number type\n",
    "        self.delta = error*self.derivative()\n",
    "\n",
    "    # calculate neuron's delta when back propagation for hidden layer\n",
    "    def hidden_layer_delta(self,weight,next_delta):\n",
    "        self.delta = self.derivative()*weight*next_delta\n",
    "\n",
    "    # calculate Neuron's output, g(W*a+bias)\n",
    "    def calculate_out(self, inputs):\n",
    "        self.input = inputs\n",
    "        self.input_shape = inputs.shape\n",
    "        self.output = self.logistic_regression(self.calculate_in())\n",
    "        self.output_shape = (1,1)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    # calculate W*a + bias\n",
    "    def calculate_in(self):                \n",
    "        # inputs[1] and weights[1] are DataFrame\n",
    "        return np.dot(self.input, self.weights)[0][0] + self.bias\n",
    "\n",
    "    # calculate g'(a)\n",
    "    def derivative(self,epsilon = 0.001):\n",
    "    \n",
    "        return (self.logistic_regression(self.calculate_in()+epsilon) - \\\n",
    "                self.logistic_regression(self.calculate_in()-epsilon))/(2*epsilon)\n",
    "\n",
    "    # logistic function, well it shouldn't be logistic_regression, rather it's logistic\n",
    "    def logistic_regression(self, input):\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Layer objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each Layer object contains neuron_num numbers of neuron objects\n",
    "class Layer:\n",
    "    # neuron_num is the number of neurons within this layer\n",
    "    # weight_num is how many weights for each neuron\n",
    "    def __init__(self, neuron_num, weight_num, bias=1):\n",
    "\n",
    "        # default bias = 1\n",
    "        self.bias = bias \n",
    "        # list of neurons in this layer\n",
    "        self.neurons = []\n",
    "        # how many neurons \n",
    "        self.neuron_num= neuron_num\n",
    "        # self.output is the output that feeds to the next layer\n",
    "        self.output = None\n",
    "        # self.weight_num is how many weights for each neuron in this layer\n",
    "        self.weight_num = weight_num\n",
    "    \n",
    "        # random initialize weights Xavier Initialization\n",
    "        for i in range(neuron_num):\n",
    "            self.neurons.append(Neuron(self.weight_num,np.sqrt(6/(self.neuron_num+self.neuron_num)),self.bias))\n",
    "\n",
    "    # calculate the output for each neuron\n",
    "    # store it in self.output\n",
    "    def feed_forward(self, inputs):\n",
    "        self.output = pd.DataFrame([])\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            self.output = self.output.append([neuron.calculate_out(inputs)],ignore_index=True)\n",
    "        \n",
    "    # return output to next layer\n",
    "    def feed_output(self):\n",
    "        return self.output   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## please read this page! \n",
    "\n",
    "## Assumptions and Discussion about Question 5 \n",
    "\n",
    "this is ANN, artificial neuron network. There is much to talk about this object. \n",
    "\n",
    "This ANN is not really like how the homework wants me to do. Specifically, A single ANN object is not able to take in a bunch of images and tell which image is what digit, rather 10 ANN objects are needed and each ANN object can only be used to tell how likely for an image to be a specific digit. \n",
    "\n",
    "I took a look at many ANN example, but I was confused how can I use a network with more than one output neuron in the output layer. But, my way of implementation can actually form a large network which can guess what an image would be if there are 10 ANN objects get trained, each of which tells the likeliness for an image to be 1,2,3.. etc, only with the cost of excessive computation. \n",
    "\n",
    "I'm sure there is a way to train a large network that can tell the handwritting for each picture, but I don't quite understand how it works. So for question 6, I will use a network like this: \n",
    "\n",
    "\n",
    "0. There are 10 ANN objects, each is responsible for telling how likely an input image would be a digit. So for example, when an image will be put into 10 ANN objects, the first ANN object tell how likely is it to be \"0\", second tells how likely is it to be a \"1\"....etc.  \n",
    "1. there are 3 layers in each ANN:input, output and one hidden layer. \n",
    "2. input layer: take as input a bunch of images, each of which is a row of record in MNIST. \n",
    "3. hidden layer: number of nodes are specified when calling the constructor.\n",
    "4. output layer: contains just one neuron, whose output is a likeliness tells percentage of likeliness the input picture is one of (1,2,3,4,5,6,7,8,9).  So if I want to test a image dataset datasets with 9 labels, then I will need to train 9 ANN objects and put the image to each of them. This is maybe taking more time to train and test, and sounds like a silly way to do it. \n",
    "\n",
    "So if you want to test out ANN, remember that an ANN object can only tell how likely an image is one of [1,2,3,4,5,6,7,8,9,0], depending only which one of [1,2,3,4,5,6,7,8,9,0] you train it to tell.\n",
    "\n",
    "I will provide a auxiliary function to help test MNIST in question 6. For now please don't expect it to be exactly like the homework's specification. \n",
    "\n",
    "Below is ANN objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN():\n",
    "    \n",
    "    # h number of hidden layer\n",
    "    # s number of nodes in each layer\n",
    "    def __init__(self,s,h=1):\n",
    "        \n",
    "        # y_hat: predicted value \n",
    "        self.y_hat = None\n",
    "        # y: input \n",
    "        self.y = None\n",
    "        # number of HIDDEN+output layers, besides input layer\n",
    "        self.h = h\n",
    "        # number of nodes in each layer\n",
    "        self.s = s\n",
    "        # learning rate\n",
    "        self.alpha = None \n",
    "        #input \n",
    "        self.X = None\n",
    "        # number of iterations\n",
    "        self.t = None\n",
    "        # number of input images\n",
    "        self.num_input = None\n",
    "        # number of output labels\n",
    "        self.num_output = None\n",
    "        # num_column is how many columns in self.xX\n",
    "        self.num_column = None\n",
    "        # a list of hidden layers\n",
    "        self.hidden_layers = []\n",
    "        # output neuron \n",
    "        self.output_neuron = Neuron(num_weight = self.s,b = np.sqrt(6/(1+self.s)))\n",
    "        \n",
    "            \n",
    "    # print(h=1,s=1) prints out weights of 1st node in hidden layer 1\n",
    "    # print(h=1) prints out all nodes's weights in layer 1\n",
    "    # print(s=1) prints out 1st node in all layers\n",
    "    def print(self,h=None,s=None):\n",
    "\n",
    "        # print weight of all nodes in the current ANN\n",
    "        if h is None and s is None:\n",
    "            for i in range(len(self.hidden_layers)):\n",
    "                for j in range(len(self.hidden_layers[i].neurons)):\n",
    "                    print(\"Weights in\",j+1, \"\\bth neuron in\",i+1,\"\\bth hidden layer: \\n\",\\\n",
    "                          self.get_node_weight(i,j).T.values,\"\\n\")\n",
    "        \n",
    "        # print weights of nodes in a specified layer\n",
    "        if h is not None and s is None:\n",
    "            for j in range(len(self.hidden_layers[h].neurons)):\n",
    "                print(\"Weights in\",j+1, \"\\bth neuron in\",h+1,\"\\bth hidden layer: \\n\",\\\n",
    "                          self.get_node_weight(h,j).T.values,\"\\n\")\n",
    "    \n",
    "        if h is not None and s is not None:\n",
    "            print(\"Weights in\",s+1, \"\\bth neuron in\",h+1,\"\\bth hidden layer: \\n\",\\\n",
    "                          self.get_node_weight(h,s).T.values,\"\\n\") \n",
    "        \n",
    "    \n",
    "    # get weights of the s-th node in the h-th hiddent layer\n",
    "    # both starting from 1! \n",
    "    def get_node_weight(self,h,s):\n",
    "        \n",
    "        if h < 0 or s < 0:\n",
    "            print(\"h and s can not be smaller than 1\")\n",
    "            return \n",
    "        \n",
    "        if h > self.h -1 :\n",
    "            print(\"Invalid number of hidden layer, max level of h(layers): \",self.h-1)\n",
    "            return \n",
    "        \n",
    "        if s > self.s-1:\n",
    "            print(\"Invalid number of node, max level of s(nodes): \",self.s-1)\n",
    "            return \n",
    "        \n",
    "        return self.hidden_layers[h].neurons[s].weights\n",
    "\n",
    "    # return weights on the output neuro\n",
    "    def get_output_neuro_weight(self):\n",
    "        return self.output_neuron.weights\n",
    "\n",
    "    # fit a model. \n",
    "    def fit(self,X,y,alpha,t):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.alpha =alpha\n",
    "        self.t = t\n",
    "        # number of input images\n",
    "        self.num_input = self.X.shape[0]\n",
    "        # number of output labels\n",
    "        self.num_output = self.X.shape[0]\n",
    "        self.num_column = self.X.shape[1]\n",
    "\n",
    "        # initiating hidden layer and weights of its neurons\n",
    "        for i in range(self.h):\n",
    "            self.hidden_layers.append(Layer(self.s,self.num_column))\n",
    "\n",
    "        self.back_propagation()\n",
    "\n",
    "    # take an image and output a guess. \n",
    "    def predict(self,T):\n",
    "        \n",
    "        y_hat = pd.DataFrame([])\n",
    "        \n",
    "        # loop through each example\n",
    "        for j in range(self.X.shape[0]):\n",
    "            \n",
    "            # aviod pass-by-reference\n",
    "            next_input = self.X.iloc[j:j+1,:].copy()\n",
    "            \n",
    "            # for each example, do the forward propagation\n",
    "            for i in self.hidden_layers:\n",
    "                i.feed_forward(next_input)\n",
    "                next_input = i.feed_output().copy()\n",
    "            \n",
    "            self.output_neuron.calculate_out(next_input.T)\n",
    "                \n",
    "            y_hat = y_hat.append(pd.DataFrame([self.output_neuron.output]),ignore_index=True)\n",
    "        return y_hat\n",
    "                \n",
    "        \n",
    "    # back propagation     \n",
    "    def back_propagation(self):\n",
    "        \n",
    "        # number of iterations \n",
    "        for i in range(self.t):\n",
    "            \n",
    "            # loop through each example\n",
    "            for j in range(self.X.shape[0]):\n",
    "                \n",
    "                # aviod pass-by-reference\n",
    "                next_input = self.X.iloc[j:j+1,:].copy()\n",
    "                \n",
    "                # for each example, do the forward propagation\n",
    "                for i in self.hidden_layers:\n",
    "                    i.feed_forward(next_input)\n",
    "                    next_input = i.feed_output().copy()\n",
    "                \n",
    "                self.output_neuron.calculate_out(next_input.T)\n",
    "                  \n",
    "                # back propagation output Neuron\n",
    "                self.y_hat = pd.DataFrame([self.output_neuron.output])\n",
    "                # error is a number \n",
    "                self.error = (self.y.iloc[j,:][0] - self.y_hat)[0][0]\n",
    "\n",
    "                #self.before_weights = self.output_neuron.weights.copy()\n",
    "                self.output_neuron.output_neuron_delta(self.error)             \n",
    "                \n",
    "                #print(0.5*(self.error)**2)\n",
    "                \n",
    "                # Since Question 6 asks to avoid cases with more than 1 \n",
    "                # hidden layer, I will here try to be lazy here, this only\n",
    "                # works for case with only 1 hidden layer \n",
    "                for i in range(len(self.hidden_layers[0].neurons)):                 \n",
    "                    \n",
    "                    # update hidden layer's weights\n",
    "                    self.hidden_layers[0].neurons[i].hidden_layer_delta(\\\n",
    "                    self.output_neuron.weights.iloc[i,:][0],self.output_neuron.delta)\n",
    "                    \n",
    "                    change = (self.alpha*self.hidden_layers[0].neurons[i].input*\\\n",
    "                    self.hidden_layers[0].neurons[i].delta).T\n",
    "                    \n",
    "                    change.reset_index(drop=True,inplace=True)\n",
    "                    change.columns=[0]\n",
    "                      \n",
    "                    self.hidden_layers[0].neurons[i].weights =  \\\n",
    "                    self.hidden_layers[0].neurons[i].weights + change\n",
    "                \n",
    "                    self.before = self.output_neuron.weights.copy()\n",
    "                \n",
    "                    # update the output neuron's weights\n",
    "                    self.output_neuron.weights = self.output_neuron.weights + \\\n",
    "                         (self.alpha*self.output_neuron.input*\\\n",
    "                         self.output_neuron.delta).T\n",
    "                          \n",
    "                    #print(self.output_neuron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "## Notice: you must use exactly train.csv and test.csv from the kaggle project link given by the homework description. Using the original MNIST dataset will not be ok.  \n",
    "Given the weird design that mentioned in Question (sorry again for it). I will need an auxiliary functionallities(which is below) to put things together so that when a dataset of images is given, a similar output required by predict() function in Question 5 will be output. \n",
    "\n",
    "some empirical testing experience: The speed depends on the number of training images, size of hidden layer and most heavily on number of iterations. On my own testing, an ANN object takes about 2 minutes to train a dataset with 500 images and 10 iterations, with learning rate = 0.05 using a single hidden layer with 10 nodes. But this learning rate is inferior than a rate of 0.01, combined with 50 iterations in terms of accuracy rate. So I would take a training size 500 images for each ANN model with iteration about 10 - 50 times. iteration 0.01 - 0.05. Each ANN model takes about 2 minutes to train, and there are 10 ANN models needed (each is responsible for telling the likeliness of resembling a single digits).\n",
    "\n",
    "So the following test parameters works better in terms of accuracy for each ANN object: \n",
    "\n",
    "       iteration: 20 - 30   \n",
    "       training_size: 500 or more\n",
    "       learning: 0.01\n",
    "       nodes in hidden layer: 10 or more\n",
    "       \n",
    "Totally, there 10 ANN objects needed, on my own test, the above case will need 20 minutes to train. Please consider how much time you would spend on grading each homework and adjust parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training model for digit:  0\n",
      "Finished training model for digit:  0\n",
      "Time used: 847.6264219284058\n",
      "Now training model for digit:  1\n",
      "Finished training model for digit:  1\n",
      "Time used: 839.7619171142578\n",
      "Now training model for digit:  2\n",
      "Finished training model for digit:  2\n",
      "Time used: 847.0323033332825\n",
      "Now training model for digit:  3\n",
      "Finished training model for digit:  3\n",
      "Time used: 842.4923000335693\n",
      "Now training model for digit:  4\n",
      "Finished training model for digit:  4\n",
      "Time used: 942.9699575901031\n",
      "Now training model for digit:  5\n",
      "Finished training model for digit:  5\n",
      "Time used: 965.4952714443207\n",
      "Now training model for digit:  6\n",
      "Finished training model for digit:  6\n",
      "Time used: 1018.8610332012177\n",
      "Now training model for digit:  7\n",
      "Finished training model for digit:  7\n",
      "Time used: 932.4615404605865\n",
      "Now training model for digit:  8\n",
      "Finished training model for digit:  8\n",
      "Time used: 845.6873750686646\n",
      "Now training model for digit:  9\n",
      "Finished training model for digit:  9\n",
      "Time used: 989.1709711551666\n"
     ]
    }
   ],
   "source": [
    "# Let's train 10 ANN objects, each of which is trained to tell the likeliness an image is one of [0,1,2,3,4,5,6,7,8,9].\n",
    "\n",
    "# input is a dataframe input\n",
    "\n",
    "# original MNIST data in CSV files.\n",
    "\n",
    "# models is a list to contain 10 trained ANN objects.\n",
    "# for now assume that we are using just 1 hidden layer\n",
    "import time\n",
    "\n",
    "def train_model(raw_file, models,iteration,training_size,learning_rate,node_in_hidden_layer):\n",
    "    \n",
    "    raw = pd.read_csv(raw_file)  \n",
    "    # this list contains 10 trained ANN objects that can be used to predict\n",
    "\n",
    "    for i in range(10):\n",
    "        \n",
    "        print(\"Now training model for digit: \", i)\n",
    "        before= time.time()\n",
    "        \n",
    "        # random choose sample images from original dataset\n",
    "        # to be used as training images for model \n",
    "        a = raw[raw['label']==i].sample(training_size*2) \n",
    "        a.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        na = raw[raw['label']!=i].sample(training_size*2) \n",
    "        na.reset_index(drop=True,inplace=True)\n",
    "        na.iloc[:,0:1] =0\n",
    "\n",
    "        c = pd.concat([a,na])\n",
    "        c.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        random_c = c.sample(training_size)\n",
    "        random_c.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        # end of random sampling\n",
    "        \n",
    "        # X is input X \n",
    "        # y is actual labels of X \n",
    "        X = random_c.iloc[:,1:]\n",
    "        y = random_c.iloc[:,0:1]\n",
    "        \n",
    "        # train model   \n",
    "        models.append(ANN(s=node_in_hidden_layer))\n",
    "        models[i].fit(X,y,learning_rate,iteration)\n",
    "        \n",
    "        print(\"Finished training model for digit: \", i)\n",
    "        print(\"Time used:\", time.time()-before)\n",
    "        \n",
    "### train models ###\n",
    "\n",
    "model = []\n",
    "\n",
    "train_model(raw_file =\"C:\\\\Users\\\\zheny\\\\Downloads\\\\train.csv\", \\\n",
    "            models = model,iteration=10,training_size=500,learning_rate=0.01,node_in_hidden_layer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b08b0d8f6ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;31m# y is label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;31m# model is a list contains the models trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b08b0d8f6ff6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X, y, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0meach_model\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mrow_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrow_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b5d4c50bfbb>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, T)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[1;31m# for each example, do the forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mnext_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-305414272489>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[1;31m# return output to next layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   4327\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4328\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4329\u001b[0;31m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4330\u001b[0m                 \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   2095\u001b[0m                                  'backfill or nearest reindexing')\n\u001b[1;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2097\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_platform_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_indexer (pandas\\index.c:5980)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.lookup (pandas\\hashtable.c:7663)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m \u001b[1;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \"\"\"Convert the input to an array.\n\u001b[1;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def predict(X,y,model):\n",
    "    result = pd.DataFrame([])\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        row_result = []\n",
    "        \n",
    "        for each_model in model:\n",
    "            row_result.append(each_model.predict(X.iloc[i:i+1,:])[0][0])            \n",
    "\n",
    "        row_result.append(y.iloc[i,:][0])\n",
    "        row_result = pd.DataFrame([row_result])\n",
    "        result = pd.concat([result,row_result],ignore_index=True)\n",
    "        \n",
    "    return result\n",
    "\n",
    "# prepare for test data \n",
    "# random choose 100 images from MNIST and try to predict it\n",
    "\n",
    "raw = pd.read_csv(\"C:\\\\Users\\\\zheny\\\\Downloads\\\\train.csv\")\n",
    "test = raw.sample(100) \n",
    "test.reset_index(drop=True,inplace=True)\n",
    "X = test.iloc[:,1:]\n",
    "y= test.iloc[:,0:1]\n",
    "\n",
    "# X is input \n",
    "# y is label\n",
    "# model is a list contains the models trained \n",
    "predict(X,y,model) # predict should relatively quick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
